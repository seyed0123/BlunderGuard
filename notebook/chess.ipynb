{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "809f110b",
   "metadata": {
    "papermill": {
     "duration": 0.008652,
     "end_time": "2025-12-16T14:15:42.578387",
     "exception": false,
     "start_time": "2025-12-16T14:15:42.569735",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# prossesing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28159781",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T14:15:42.594312Z",
     "iopub.status.busy": "2025-12-16T14:15:42.594069Z",
     "iopub.status.idle": "2025-12-16T14:15:43.997580Z",
     "shell.execute_reply": "2025-12-16T14:15:43.996888Z"
    },
    "papermill": {
     "duration": 1.412803,
     "end_time": "2025-12-16T14:15:43.998914",
     "exception": false,
     "start_time": "2025-12-16T14:15:42.586111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>surahs</th>\n",
       "      <th>ayahs</th>\n",
       "      <th>ayahs-translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>In the name of Allah, Most Gracious, Most Merc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Praise be to Allah, the Cherisher and Sustaine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Most Gracious, Most Merciful;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Master of the Day of Judgment.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Thee do we worship, and Thine aid we seek.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  surahs  ayahs                                  ayahs-translation\n",
       "0   1       1      1  In the name of Allah, Most Gracious, Most Merc...\n",
       "1   2       1      2  Praise be to Allah, the Cherisher and Sustaine...\n",
       "2   3       1      3                      Most Gracious, Most Merciful;\n",
       "3   4       1      4                     Master of the Day of Judgment.\n",
       "4   5       1      5         Thee do we worship, and Thine aid we seek."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/kaggle/input/the-holy-quran-english-translation-dataset/holy_quran-english.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8793be43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T14:15:44.016694Z",
     "iopub.status.busy": "2025-12-16T14:15:44.016198Z",
     "iopub.status.idle": "2025-12-16T14:15:44.022027Z",
     "shell.execute_reply": "2025-12-16T14:15:44.021237Z"
    },
    "papermill": {
     "duration": 0.015619,
     "end_time": "2025-12-16T14:15:44.023111",
     "exception": false,
     "start_time": "2025-12-16T14:15:44.007492",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "903673\n"
     ]
    }
   ],
   "source": [
    "quran = \"\\n\".join(df[\"ayahs-translation\"].astype(str).tolist())\n",
    "print(len(quran))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8652ed88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T14:15:44.039238Z",
     "iopub.status.busy": "2025-12-16T14:15:44.038858Z",
     "iopub.status.idle": "2025-12-16T14:15:44.043056Z",
     "shell.execute_reply": "2025-12-16T14:15:44.042542Z"
    },
    "id": "2c2fc423",
    "outputId": "979cf87d-2a98-4bd2-fbac-0121b05c46f6",
    "papermill": {
     "duration": 0.01322,
     "end_time": "2025-12-16T14:15:44.044068",
     "exception": false,
     "start_time": "2025-12-16T14:15:44.030848",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "def load_all_txt(input_dir):\n",
    "    all_text = \"\"\n",
    "\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(input_dir, filename)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                all_text += f.read() + \"\\n\" +quran + '\\n'\n",
    "\n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15bc781c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T14:15:44.060264Z",
     "iopub.status.busy": "2025-12-16T14:15:44.060085Z",
     "iopub.status.idle": "2025-12-16T14:15:44.296462Z",
     "shell.execute_reply": "2025-12-16T14:15:44.295585Z"
    },
    "papermill": {
     "duration": 0.245684,
     "end_time": "2025-12-16T14:15:44.297676",
     "exception": false,
     "start_time": "2025-12-16T14:15:44.051992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10978120\n"
     ]
    }
   ],
   "source": [
    "input_directory = r\"/kaggle/input/chess-analyse-books\"\n",
    "full_text = load_all_txt(input_directory)\n",
    "print(len(full_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ed870b",
   "metadata": {
    "papermill": {
     "duration": 0.007572,
     "end_time": "2025-12-16T14:15:44.313351",
     "exception": false,
     "start_time": "2025-12-16T14:15:44.305779",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# load and test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2437977",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T14:15:44.329730Z",
     "iopub.status.busy": "2025-12-16T14:15:44.329122Z",
     "iopub.status.idle": "2025-12-16T14:17:02.191464Z",
     "shell.execute_reply": "2025-12-16T14:17:02.190727Z"
    },
    "papermill": {
     "duration": 77.871923,
     "end_time": "2025-12-16T14:17:02.192884",
     "exception": false,
     "start_time": "2025-12-16T14:15:44.320961",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\r\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\r\n",
      "Collecting pyarrow==14.0.2\r\n",
      "  Downloading pyarrow-14.0.2-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\r\n",
      "Collecting datasets==2.18.0\r\n",
      "  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\r\n",
      "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.11/dist-packages (from pyarrow==14.0.2) (1.26.4)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (3.19.1)\r\n",
      "Collecting pyarrow-hotfix (from datasets==2.18.0)\r\n",
      "  Downloading pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\r\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets==2.18.0)\r\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\r\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (2.2.3)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (2.32.5)\r\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (4.67.1)\r\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (3.5.0)\r\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (0.70.16)\r\n",
      "Collecting fsspec<=2024.2.0,>=2023.1.0 (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets==2.18.0)\r\n",
      "  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\r\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (3.12.15)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (1.0.0rc2)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (25.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (6.0.3)\r\n",
      "Collecting huggingface-hub>=0.19.4 (from datasets==2.18.0)\r\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.9.18)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\r\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.1.0)\r\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (2.6.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (1.4.0)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (25.3.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (1.7.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (6.6.4)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (0.3.2)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (1.20.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.4->datasets==2.18.0) (4.15.0)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.4->datasets==2.18.0) (1.1.10)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.6->pyarrow==14.0.2) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.6->pyarrow==14.0.2) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.6->pyarrow==14.0.2) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.6->pyarrow==14.0.2) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.6->pyarrow==14.0.2) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.6->pyarrow==14.0.2) (2.4.1)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.18.0) (3.4.3)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.18.0) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.18.0) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.18.0) (2025.8.3)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\r\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\r\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\r\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\r\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\r\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\r\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\r\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\r\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\r\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\r\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\r\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\r\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.18.0) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.18.0) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.18.0) (2025.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.18.0) (1.17.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.16.6->pyarrow==14.0.2) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.16.6->pyarrow==14.0.2) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.16.6->pyarrow==14.0.2) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.16.6->pyarrow==14.0.2) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.16.6->pyarrow==14.0.2) (2024.2.0)\r\n",
      "Downloading pyarrow-14.0.2-cp311-cp311-manylinux_2_28_x86_64.whl (38.0 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading datasets-2.18.0-py3-none-any.whl (510 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m117.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.7-py3-none-any.whl (7.9 kB)\r\n",
      "Installing collected packages: pyarrow-hotfix, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface-hub, nvidia-cusolver-cu12, pyarrow, datasets\r\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\r\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-curand-cu12\r\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\r\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\r\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\r\n",
      "  Attempting uninstall: nvidia-cufft-cu12\r\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\r\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\r\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\r\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\r\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\r\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\r\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cublas-cu12\r\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\r\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\r\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\r\n",
      "  Attempting uninstall: fsspec\r\n",
      "    Found existing installation: fsspec 2025.9.0\r\n",
      "    Uninstalling fsspec-2025.9.0:\r\n",
      "      Successfully uninstalled fsspec-2025.9.0\r\n",
      "  Attempting uninstall: dill\r\n",
      "    Found existing installation: dill 0.4.0\r\n",
      "    Uninstalling dill-0.4.0:\r\n",
      "      Successfully uninstalled dill-0.4.0\r\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\r\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\r\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\r\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\r\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\r\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\r\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\r\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\r\n",
      "  Attempting uninstall: huggingface-hub\r\n",
      "    Found existing installation: huggingface-hub 1.0.0rc2\r\n",
      "    Uninstalling huggingface-hub-1.0.0rc2:\r\n",
      "      Successfully uninstalled huggingface-hub-1.0.0rc2\r\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\r\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\r\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\r\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\r\n",
      "  Attempting uninstall: pyarrow\r\n",
      "    Found existing installation: pyarrow 19.0.1\r\n",
      "    Uninstalling pyarrow-19.0.1:\r\n",
      "      Successfully uninstalled pyarrow-19.0.1\r\n",
      "  Attempting uninstall: datasets\r\n",
      "    Found existing installation: datasets 4.1.1\r\n",
      "    Uninstalling datasets-4.1.1:\r\n",
      "      Successfully uninstalled datasets-4.1.1\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\r\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\r\n",
      "bigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\r\n",
      "bigframes 2.12.0 requires pyarrow>=15.0.2, but you have pyarrow 14.0.2 which is incompatible.\r\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\r\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\r\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\r\n",
      "cudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\r\n",
      "pandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\r\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed datasets-2.18.0 dill-0.3.8 fsspec-2024.2.0 huggingface-hub-0.36.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pyarrow-14.0.2 pyarrow-hotfix-0.7\r\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers accelerate pyarrow==14.0.2 datasets==2.18.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f6661f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T14:17:02.252585Z",
     "iopub.status.busy": "2025-12-16T14:17:02.252144Z",
     "iopub.status.idle": "2025-12-16T14:17:45.444921Z",
     "shell.execute_reply": "2025-12-16T14:17:45.444168Z"
    },
    "papermill": {
     "duration": 43.223283,
     "end_time": "2025-12-16T14:17:45.446120",
     "exception": false,
     "start_time": "2025-12-16T14:17:02.222837",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "866ad452277441bf9b5dee66e79389b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73a494991c314a598c9dc52ac593b546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1d20086caf84e7ea09226329e6aa870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a899d32de2e46d5b2c760d5c2f4fcef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "593fdf92be234e4291d136980d6a1951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/726 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "2025-12-16 14:17:27.102904: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765894647.307962      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765894647.361598      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6359802ba2384ce4955bc3df70bc68c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "567d3ed6fa70489d83fe28795aae6328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList\n",
    "import torch\n",
    "\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"left\"\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99472fa8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T14:17:45.508957Z",
     "iopub.status.busy": "2025-12-16T14:17:45.508385Z",
     "iopub.status.idle": "2025-12-16T14:17:45.515873Z",
     "shell.execute_reply": "2025-12-16T14:17:45.515302Z"
    },
    "papermill": {
     "duration": 0.040154,
     "end_time": "2025-12-16T14:17:45.516984",
     "exception": false,
     "start_time": "2025-12-16T14:17:45.476830",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StopAtSentence(StoppingCriteria):\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        # Check last token\n",
    "        last_token = input_ids[0][-1].item()\n",
    "        # Stop if EOS or common sentence-ending token (\".\", \"?\", \"!\")\n",
    "        if last_token == tokenizer.eos_token_id:\n",
    "            return True\n",
    "        decoded = tokenizer.decode([last_token], skip_special_tokens=True)\n",
    "        if decoded.strip() in {\".\", \"?\", \"!\"}:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "def chat_qwen3_direct(prompt: str, max_new_tokens: int = 256) -> str:\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    # Apply chat template WITHOUT thinking\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False  # ğŸ”¥ Disable thinking for factual answers\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.5,      # Lower = more focused\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            # stopping_criteria=StoppingCriteriaList([StopAtSentence()])  # Optional\n",
    "        )\n",
    "    \n",
    "    # Extract only the new text\n",
    "    input_len = inputs.input_ids.shape[1]\n",
    "    response = tokenizer.decode(outputs[0][input_len:], skip_special_tokens=True)\n",
    "    \n",
    "    # Clean: remove any trailing incomplete sentence\n",
    "    for end in [\".\", \"?\", \"!\"]:\n",
    "        if end in response:\n",
    "            response = response[:response.rfind(end)+1]\n",
    "            break\n",
    "    else:\n",
    "        # No sentence end? Return first line or truncate\n",
    "        response = response.split(\"\\n\")[0].strip()\n",
    "    \n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75da14f1",
   "metadata": {
    "papermill": {
     "duration": 0.029829,
     "end_time": "2025-12-16T14:17:45.576740",
     "exception": false,
     "start_time": "2025-12-16T14:17:45.546911",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test the model output on simple question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e511cdeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T14:17:45.660117Z",
     "iopub.status.busy": "2025-12-16T14:17:45.659447Z",
     "iopub.status.idle": "2025-12-16T14:17:48.433286Z",
     "shell.execute_reply": "2025-12-16T14:17:48.432360Z"
    },
    "papermill": {
     "duration": 2.817654,
     "end_time": "2025-12-16T14:17:48.434774",
     "exception": false,
     "start_time": "2025-12-16T14:17:45.617120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¬ Answer: Controlling the center is crucial in chess because it provides a flexible and dynamic opening, allowing players to adapt their strategies and create a strong, attacking position. It also enables a wide range of moves, making it a central aspect of both opening and closing play.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Explain why controlling the center is important in chess in 2â€“3 sentences.\"\n",
    "answer = chat_qwen3_direct(prompt)\n",
    "print(\"ğŸ’¬ Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db99f9a",
   "metadata": {
    "papermill": {
     "duration": 0.03016,
     "end_time": "2025-12-16T14:17:48.499162",
     "exception": false,
     "start_time": "2025-12-16T14:17:48.469002",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# fine tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53dd423a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T14:17:48.561958Z",
     "iopub.status.busy": "2025-12-16T14:17:48.561497Z",
     "iopub.status.idle": "2025-12-16T14:17:48.566493Z",
     "shell.execute_reply": "2025-12-16T14:17:48.565923Z"
    },
    "papermill": {
     "duration": 0.037278,
     "end_time": "2025-12-16T14:17:48.567498",
     "exception": false,
     "start_time": "2025-12-16T14:17:48.530220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chunk_text(text, tokenizer, max_length):\n",
    "    # Use batched tokenization to avoid huge tensors\n",
    "    tokens = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=False,  # Avoid extra BOS/EOS in middle of book\n",
    "        truncation=False,\n",
    "        verbose=False\n",
    "    )[\"input_ids\"][0]\n",
    "\n",
    "    chunks = []\n",
    "    # Use full chunks only (avoid tiny trailing fragments)\n",
    "    for i in range(0, len(tokens) - max_length + 1, max_length):\n",
    "        chunk = tokens[i:i + max_length].tolist()\n",
    "        chunks.append({\"input_ids\": chunk})\n",
    "    \n",
    "    # Optional: add last partial chunk if long enough\n",
    "    remainder = len(tokens) % max_length\n",
    "    if remainder > 64:  # only if >64 tokens\n",
    "        chunks.append({\"input_ids\": tokens[-remainder:].tolist()})\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b76d86c",
   "metadata": {
    "papermill": {
     "duration": 0.030788,
     "end_time": "2025-12-16T14:17:48.631544",
     "exception": false,
     "start_time": "2025-12-16T14:17:48.600756",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## prepare the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b745c1bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T14:17:48.691956Z",
     "iopub.status.busy": "2025-12-16T14:17:48.691703Z",
     "iopub.status.idle": "2025-12-16T14:18:05.145053Z",
     "shell.execute_reply": "2025-12-16T14:18:05.144380Z"
    },
    "papermill": {
     "duration": 16.485304,
     "end_time": "2025-12-16T14:18:05.146394",
     "exception": false,
     "start_time": "2025-12-16T14:17:48.661090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.Tensor size changed, may indicate binary incompatibility. Expected 64 from C header, got 80 from PyObject\n",
      "<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.ChunkedArray size changed, may indicate binary incompatibility. Expected 64 from C header, got 72 from PyObject\n",
      "<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib._Tabular size changed, may indicate binary incompatibility. Expected 24 from C header, got 32 from PyObject\n",
      "<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.Table size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n",
      "<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.NativeFile size changed, may indicate binary incompatibility. Expected 96 from C header, got 104 from PyObject\n",
      "<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.BufferedInputStream size changed, may indicate binary incompatibility. Expected 96 from C header, got 104 from PyObject\n",
      "<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.BufferedOutputStream size changed, may indicate binary incompatibility. Expected 96 from C header, got 104 from PyObject\n",
      "<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.CompressedInputStream size changed, may indicate binary incompatibility. Expected 96 from C header, got 104 from PyObject\n",
      "<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.CompressedOutputStream size changed, may indicate binary incompatibility. Expected 96 from C header, got 104 from PyObject\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset\n",
    "\n",
    "max_length = 256\n",
    "\n",
    "data = chunk_text(full_text, tokenizer, max_length)\n",
    "dataset = Dataset.from_list(data)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,                              \n",
    "    pad_to_multiple_of=8                    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31e57bf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T14:18:05.212245Z",
     "iopub.status.busy": "2025-12-16T14:18:05.211185Z",
     "iopub.status.idle": "2025-12-16T14:18:05.254262Z",
     "shell.execute_reply": "2025-12-16T14:18:05.253711Z"
    },
    "papermill": {
     "duration": 0.076504,
     "end_time": "2025-12-16T14:18:05.255385",
     "exception": false,
     "start_time": "2025-12-16T14:18:05.178881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./chess-qwen\",\n",
    "    run_name=\"chess-qwen-run-1\",\n",
    "\n",
    "    \n",
    "    per_device_train_batch_size=2,    \n",
    "    gradient_accumulation_steps=16,   \n",
    "\n",
    "   \n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,                \n",
    "    fp16=True,                      \n",
    "    bf16=False,                        \n",
    "\n",
    "    \n",
    "    save_steps=2000,                   \n",
    "    logging_steps=200,                  \n",
    "    save_total_limit=1,                 \n",
    "\n",
    "    # ğŸš€ Other speedups\n",
    "    optim=\"adamw_torch\",                \n",
    "    report_to=\"none\",                  \n",
    "    dataloader_num_workers=4,           \n",
    "    remove_unused_columns=True,\n",
    ")\n",
    "\n",
    "\n",
    "# 5. Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d452a72",
   "metadata": {
    "papermill": {
     "duration": 0.029503,
     "end_time": "2025-12-16T14:18:05.315262",
     "exception": false,
     "start_time": "2025-12-16T14:18:05.285759",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dca9463a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T14:18:05.375536Z",
     "iopub.status.busy": "2025-12-16T14:18:05.375295Z",
     "iopub.status.idle": "2025-12-16T16:29:53.045006Z",
     "shell.execute_reply": "2025-12-16T16:29:53.044145Z"
    },
    "papermill": {
     "duration": 7907.734275,
     "end_time": "2025-12-16T16:29:53.079241",
     "exception": false,
     "start_time": "2025-12-16T14:18:05.344966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1185' max='1185' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1185/1185 2:11:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.639600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.761900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.299200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.227600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.846700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./chess-qwen-adapted/tokenizer_config.json',\n",
       " './chess-qwen-adapted/special_tokens_map.json',\n",
       " './chess-qwen-adapted/chat_template.jinja',\n",
       " './chess-qwen-adapted/vocab.json',\n",
       " './chess-qwen-adapted/merges.txt',\n",
       " './chess-qwen-adapted/added_tokens.json',\n",
       " './chess-qwen-adapted/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('start training')\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "model.save_pretrained(\"./chess-qwen-adapted\")\n",
    "tokenizer.save_pretrained(\"./chess-qwen-adapted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa6ce33f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T16:29:53.147021Z",
     "iopub.status.busy": "2025-12-16T16:29:53.146263Z",
     "iopub.status.idle": "2025-12-16T16:29:53.532499Z",
     "shell.execute_reply": "2025-12-16T16:29:53.531873Z"
    },
    "papermill": {
     "duration": 0.421585,
     "end_time": "2025-12-16T16:29:53.533884",
     "exception": false,
     "start_time": "2025-12-16T16:29:53.112299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "del trainer\n",
    "del model\n",
    "del data_collator\n",
    "del dataset\n",
    "del tokenizer\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155801a6",
   "metadata": {
    "papermill": {
     "duration": 0.030612,
     "end_time": "2025-12-16T16:29:53.596107",
     "exception": false,
     "start_time": "2025-12-16T16:29:53.565495",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# test the adapted model on same simple question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93f9174f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T16:29:53.658700Z",
     "iopub.status.busy": "2025-12-16T16:29:53.658082Z",
     "iopub.status.idle": "2025-12-16T16:29:55.178102Z",
     "shell.execute_reply": "2025-12-16T16:29:55.177398Z"
    },
    "papermill": {
     "duration": 1.552968,
     "end_time": "2025-12-16T16:29:55.179215",
     "exception": false,
     "start_time": "2025-12-16T16:29:53.626247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"/kaggle/working/chess-qwen-adapted\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"left\"\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b89d62e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T16:29:55.242641Z",
     "iopub.status.busy": "2025-12-16T16:29:55.242354Z",
     "iopub.status.idle": "2025-12-16T16:30:06.106752Z",
     "shell.execute_reply": "2025-12-16T16:30:06.105863Z"
    },
    "papermill": {
     "duration": 10.897208,
     "end_time": "2025-12-16T16:30:06.108035",
     "exception": false,
     "start_time": "2025-12-16T16:29:55.210827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¬ Answer: Freezing water for usury,\n",
      "Freezing camels for usury,\n",
      "And so on. Freezing water is an abomination,\n",
      "\n",
      "Freezing wine, for usury, and so on. Freezing a variety of beverages\n",
      "Freezing camels, and oxen a yoke.\n",
      "And so on. Let usury free for him, and let him have joy from the fruits of this (artistry): for him is a great Reward.\n",
      "Freezing of cattle is an abomination,-\n",
      "And a Sign for him. And an admonition,-\n",
      "That he should not eat up his own blood, nor kill his children,\n",
      "Nor place his feet to support himself,\n",
      "But squatting in the middle of the board (with a sign of fear),-\n",
      "Should he not guard his own safety?-\n",
      "And he should not eat up his parents, or his kindred, orphans,\n",
      "Nor marry (your girls) to them, but ye should fear Allah.\n",
      "O ye who believe! enter not houses other than your own, until ye have asked permission and saluted those in them: that is best for you, in order that ye may heed (what is seemly).\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Explain why controlling the center is important in chess in 2â€“3 sentences.\"\n",
    "answer = chat_qwen3_direct(prompt)\n",
    "print(\"ğŸ’¬ Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1aadff",
   "metadata": {
    "papermill": {
     "duration": 0.030078,
     "end_time": "2025-12-16T16:30:06.170084",
     "exception": false,
     "start_time": "2025-12-16T16:30:06.140006",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Quantizing the model using llama-cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e59b3e6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T16:30:06.232444Z",
     "iopub.status.busy": "2025-12-16T16:30:06.231881Z",
     "iopub.status.idle": "2025-12-16T16:32:34.840003Z",
     "shell.execute_reply": "2025-12-16T16:32:34.839188Z"
    },
    "papermill": {
     "duration": 148.641105,
     "end_time": "2025-12-16T16:32:34.841459",
     "exception": false,
     "start_time": "2025-12-16T16:30:06.200354",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\r\n",
      "Collecting llama-cpp-python>=0.2.0\r\n",
      "  Downloading llama_cpp_python-0.3.16.tar.gz (50.7 MB)\r\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.7/50.7 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python>=0.2.0) (4.15.0)\r\n",
      "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python>=0.2.0) (1.26.4)\r\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python>=0.2.0)\r\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\r\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python>=0.2.0) (3.1.6)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python>=0.2.0) (3.0.2)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python>=0.2.0) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python>=0.2.0) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python>=0.2.0) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python>=0.2.0) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python>=0.2.0) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python>=0.2.0) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python>=0.2.0) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python>=0.2.0) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.20.0->llama-cpp-python>=0.2.0) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.20.0->llama-cpp-python>=0.2.0) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.20.0->llama-cpp-python>=0.2.0) (2024.2.0)\r\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\r\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.16-cp311-cp311-linux_x86_64.whl size=4503226 sha256=d25316061f9c72f7e0db90a92a5a0139e4a74e5926877550b46c0bde677deff9\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/d8/5b/e5/a7d4b5765da347d314e8155197440c9995a962f8e4a5f52b23\r\n",
      "Successfully built llama-cpp-python\r\n",
      "Installing collected packages: diskcache, llama-cpp-python\r\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.16\r\n",
      "Cloning into 'llama.cpp'...\r\n",
      "remote: Enumerating objects: 72321, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (297/297), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (234/234), done.\u001b[K\r\n",
      "remote: Total 72321 (delta 189), reused 63 (delta 63), pack-reused 72024 (from 4)\u001b[K\r\n",
      "Receiving objects: 100% (72321/72321), 242.37 MiB | 23.71 MiB/s, done.\r\n",
      "Resolving deltas: 100% (52225/52225), done.\r\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece\n",
    "!pip install \"llama-cpp-python>=0.2.0\"\n",
    "!git clone https://github.com/ggerganov/llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb77db66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T16:32:34.919550Z",
     "iopub.status.busy": "2025-12-16T16:32:34.918858Z",
     "iopub.status.idle": "2025-12-16T16:32:48.812516Z",
     "shell.execute_reply": "2025-12-16T16:32:48.811180Z"
    },
    "papermill": {
     "duration": 13.936141,
     "end_time": "2025-12-16T16:32:48.815717",
     "exception": false,
     "start_time": "2025-12-16T16:32:34.879576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.19G/1.19G [00:03<00:00, 319Mbyte/s]\r\n"
     ]
    }
   ],
   "source": [
    "!python llama.cpp/convert_hf_to_gguf.py \\\n",
    "    /kaggle/working/chess-qwen-adapted \\\n",
    "    --outfile /kaggle/working/chess-fp16.gguf \\\n",
    "    --outtype f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "832cf914",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T16:32:48.944122Z",
     "iopub.status.busy": "2025-12-16T16:32:48.943752Z",
     "iopub.status.idle": "2025-12-16T16:39:08.082708Z",
     "shell.execute_reply": "2025-12-16T16:39:08.081675Z"
    },
    "papermill": {
     "duration": 379.18634,
     "end_time": "2025-12-16T16:39:08.084190",
     "exception": false,
     "start_time": "2025-12-16T16:32:48.897850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- The C compiler identification is GNU 11.4.0\r\n",
      "-- The CXX compiler identification is GNU 11.4.0\r\n",
      "-- Detecting C compiler ABI info\r\n",
      "-- Detecting C compiler ABI info - done\r\n",
      "-- Check for working C compiler: /usr/bin/cc - skipped\r\n",
      "-- Detecting C compile features\r\n",
      "-- Detecting C compile features - done\r\n",
      "-- Detecting CXX compiler ABI info\r\n",
      "-- Detecting CXX compiler ABI info - done\r\n",
      "-- Check for working CXX compiler: /usr/bin/c++ - skipped\r\n",
      "-- Detecting CXX compile features\r\n",
      "-- Detecting CXX compile features - done\r\n",
      "\u001b[0mCMAKE_BUILD_TYPE=Release\u001b[0m\r\n",
      "-- Found Git: /usr/bin/git (found version \"2.34.1\")\r\n",
      "-- The ASM compiler identification is GNU\r\n",
      "-- Found assembler: /usr/bin/cc\r\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\r\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\r\n",
      "-- Found Threads: TRUE\r\n",
      "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\r\n",
      "-- CMAKE_SYSTEM_PROCESSOR: x86_64\r\n",
      "-- GGML_SYSTEM_ARCH: x86\r\n",
      "-- Including CPU backend\r\n",
      "-- Found OpenMP_C: -fopenmp (found version \"4.5\")\r\n",
      "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\r\n",
      "-- Found OpenMP: TRUE (found version \"4.5\")\r\n",
      "-- x86 detected\r\n",
      "-- Adding CPU backend variant ggml-cpu: -march=native \r\n",
      "-- ggml version: 0.9.4\r\n",
      "-- ggml commit:  ec98e2002\r\n",
      "-- Found CURL: /usr/lib/x86_64-linux-gnu/libcurl.so (found version \"7.81.0\")\r\n",
      "-- Configuring done (1.4s)\r\n",
      "-- Generating done (0.3s)\r\n",
      "-- Build files have been written to: /kaggle/working/llama.cpp/build\r\n",
      "[  0%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\r\n",
      "[  0%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\u001b[0m\r\n",
      "[  0%] \u001b[32mBuilding CXX object vendor/cpp-httplib/CMakeFiles/cpp-httplib.dir/httplib.cpp.o\u001b[0m\r\n",
      "[  0%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\u001b[0m\r\n",
      "[  0%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o\u001b[0m\r\n",
      "[  1%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\u001b[0m\r\n",
      "[  1%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o\u001b[0m\r\n",
      "[  1%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o\u001b[0m\r\n",
      "[  1%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\u001b[0m\r\n",
      "[  1%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-llava-cli.dir/deprecation-warning.cpp.o\u001b[0m\r\n",
      "[  1%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-gemma3-cli.dir/deprecation-warning.cpp.o\u001b[0m\r\n",
      "[  2%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o\u001b[0m\r\n",
      "[  2%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\u001b[0m\r\n",
      "[  2%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\u001b[0m\r\n",
      "[  2%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-qwen2vl-cli.dir/deprecation-warning.cpp.o\u001b[0m\r\n",
      "[  2%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-minicpmv-cli.dir/deprecation-warning.cpp.o\u001b[0m\r\n",
      "[  3%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\u001b[0m\r\n",
      "[  3%] Built target build_info\r\n",
      "[  3%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gemma3-cli\u001b[0m\r\n",
      "[  3%] Built target sha1\r\n",
      "[  4%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-qwen2vl-cli\u001b[0m\r\n",
      "[  4%] Built target llama-gemma3-cli\r\n",
      "[  4%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-minicpmv-cli\u001b[0m\r\n",
      "[  5%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-llava-cli\u001b[0m\r\n",
      "[  5%] Built target sha256\r\n",
      "[  5%] Built target llama-qwen2vl-cli\r\n",
      "[  5%] Built target llama-minicpmv-cli\r\n",
      "[  5%] Built target llama-llava-cli\r\n",
      "[  5%] Built target xxhash\r\n",
      "[  5%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-base.so\u001b[0m\r\n",
      "[  5%] Built target ggml-base\r\n",
      "[  5%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\u001b[0m\r\n",
      "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o\u001b[0m\r\n",
      "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\u001b[0m\r\n",
      "[  6%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o\u001b[0m\r\n",
      "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o\u001b[0m\r\n",
      "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o\u001b[0m\r\n",
      "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\u001b[0m\r\n",
      "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o\u001b[0m\r\n",
      "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\u001b[0m\r\n",
      "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o\u001b[0m\r\n",
      "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o\u001b[0m\r\n",
      "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o\u001b[0m\r\n",
      "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\u001b[0m\r\n",
      "[  8%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o\u001b[0m\r\n",
      "[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o\u001b[0m\r\n",
      "[  9%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-cpu.so\u001b[0m\r\n",
      "[  9%] Built target ggml-cpu\r\n",
      "[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\u001b[0m\r\n",
      "[ 10%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml.so\u001b[0m\r\n",
      "[ 10%] Built target ggml\r\n",
      "[ 10%] \u001b[32mBuilding CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o\u001b[0m\r\n",
      "[ 10%] \u001b[32mBuilding CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o\u001b[0m\r\n",
      "[ 10%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\r\n",
      "[ 10%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o\u001b[0m\r\n",
      "[ 10%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o\u001b[0m\r\n",
      "[ 11%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o\u001b[0m\r\n",
      "[ 11%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o\u001b[0m\r\n",
      "[ 11%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o\u001b[0m\r\n",
      "[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-cparams.cpp.o\u001b[0m\r\n",
      "[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\u001b[0m\r\n",
      "[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o\u001b[0m\r\n",
      "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o\u001b[0m\r\n",
      "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o\u001b[0m\r\n",
      "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o\u001b[0m\r\n",
      "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache-iswa.cpp.o\u001b[0m\r\n",
      "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o\u001b[0m\r\n",
      "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory.cpp.o\u001b[0m\r\n",
      "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o\u001b[0m\r\n",
      "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o\u001b[0m\r\n",
      "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o\u001b[0m\r\n",
      "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o\u001b[0m\r\n",
      "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-saver.cpp.o\u001b[0m\r\n",
      "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\u001b[0m\r\n",
      "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o\u001b[0m\r\n",
      "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\u001b[0m\r\n",
      "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\u001b[0m\r\n",
      "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\u001b[0m\r\n",
      "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\u001b[0m\r\n",
      "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/afmoe.cpp.o\u001b[0m\r\n",
      "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/apertus.cpp.o\u001b[0m\r\n",
      "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/arcee.cpp.o\u001b[0m\r\n",
      "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/arctic.cpp.o\u001b[0m\r\n",
      "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/arwkv7.cpp.o\u001b[0m\r\n",
      "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bailingmoe.cpp.o\u001b[0m\r\n",
      "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/baichuan.cpp.o\u001b[0m\r\n",
      "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bailingmoe2.cpp.o\u001b[0m\r\n",
      "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bert.cpp.o\u001b[0m\r\n",
      "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bitnet.cpp.o\u001b[0m\r\n",
      "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bloom.cpp.o\u001b[0m\r\n",
      "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/chameleon.cpp.o\u001b[0m\r\n",
      "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/chatglm.cpp.o\u001b[0m\r\n",
      "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/codeshell.cpp.o\u001b[0m\r\n",
      "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/cogvlm.cpp.o\u001b[0m\r\n",
      "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/cohere2-iswa.cpp.o\u001b[0m\r\n",
      "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/command-r.cpp.o\u001b[0m\r\n",
      "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dbrx.cpp.o\u001b[0m\r\n",
      "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deci.cpp.o\u001b[0m\r\n",
      "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deepseek.cpp.o\u001b[0m\r\n",
      "[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deepseek2.cpp.o\u001b[0m\r\n",
      "[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dots1.cpp.o\u001b[0m\r\n",
      "[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dream.cpp.o\u001b[0m\r\n",
      "[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/ernie4-5-moe.cpp.o\u001b[0m\r\n",
      "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/ernie4-5.cpp.o\u001b[0m\r\n",
      "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/exaone.cpp.o\u001b[0m\r\n",
      "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/exaone4.cpp.o\u001b[0m\r\n",
      "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/falcon-h1.cpp.o\u001b[0m\r\n",
      "[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/falcon.cpp.o\u001b[0m\r\n",
      "[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma-embedding.cpp.o\u001b[0m\r\n",
      "[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma.cpp.o\u001b[0m\r\n",
      "[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma2-iswa.cpp.o\u001b[0m\r\n",
      "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma3.cpp.o\u001b[0m\r\n",
      "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma3n-iswa.cpp.o\u001b[0m\r\n",
      "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/glm4-moe.cpp.o\u001b[0m\r\n",
      "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/glm4.cpp.o\u001b[0m\r\n",
      "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gpt2.cpp.o\u001b[0m\r\n",
      "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gptneox.cpp.o\u001b[0m\r\n",
      "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/granite-hybrid.cpp.o\u001b[0m\r\n",
      "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/granite.cpp.o\u001b[0m\r\n",
      "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/grok.cpp.o\u001b[0m\r\n",
      "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/grovemoe.cpp.o\u001b[0m\r\n",
      "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/hunyuan-dense.cpp.o\u001b[0m\r\n",
      "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/hunyuan-moe.cpp.o\u001b[0m\r\n",
      "[ 28%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/internlm2.cpp.o\u001b[0m\r\n",
      "[ 28%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/jais.cpp.o\u001b[0m\r\n",
      "[ 28%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/jamba.cpp.o\u001b[0m\r\n",
      "[ 28%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/lfm2.cpp.o\u001b[0m\r\n",
      "[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llada-moe.cpp.o\u001b[0m\r\n",
      "[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llada.cpp.o\u001b[0m\r\n",
      "[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llama-iswa.cpp.o\u001b[0m\r\n",
      "[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llama.cpp.o\u001b[0m\r\n",
      "[ 30%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mamba.cpp.o\u001b[0m\r\n",
      "[ 30%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/minicpm3.cpp.o\u001b[0m\r\n",
      "[ 30%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/minimax-m2.cpp.o\u001b[0m\r\n",
      "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/nemotron-h.cpp.o\u001b[0m\r\n",
      "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mpt.cpp.o\u001b[0m\r\n",
      "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/nemotron.cpp.o\u001b[0m\r\n",
      "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmo.cpp.o\u001b[0m\r\n",
      "[ 32%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmo2.cpp.o\u001b[0m\r\n",
      "[ 32%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/neo-bert.cpp.o\u001b[0m\r\n",
      "[ 32%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmoe.cpp.o\u001b[0m\r\n",
      "[ 32%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/openai-moe-iswa.cpp.o\u001b[0m\r\n",
      "[ 32%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/phi2.cpp.o\u001b[0m\r\n",
      "[ 32%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/openelm.cpp.o\u001b[0m\r\n",
      "[ 33%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plamo.cpp.o\u001b[0m\r\n",
      "[ 33%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plm.cpp.o\u001b[0m\r\n",
      "[ 33%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plamo2.cpp.o\u001b[0m\r\n",
      "[ 33%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/phi3.cpp.o\u001b[0m\r\n",
      "[ 33%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/pangu-embedded.cpp.o\u001b[0m\r\n",
      "[ 33%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen.cpp.o\u001b[0m\r\n",
      "[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/orion.cpp.o\u001b[0m\r\n",
      "[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2moe.cpp.o\u001b[0m\r\n",
      "[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2.cpp.o\u001b[0m\r\n",
      "[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2vl.cpp.o\u001b[0m\r\n",
      "[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3.cpp.o\u001b[0m\r\n",
      "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3vl.cpp.o\u001b[0m\r\n",
      "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3vl-moe.cpp.o\u001b[0m\r\n",
      "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3moe.cpp.o\u001b[0m\r\n",
      "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3next.cpp.o\u001b[0m\r\n",
      "[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/refact.cpp.o\u001b[0m\r\n",
      "[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rnd1.cpp.o\u001b[0m\r\n",
      "[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6-base.cpp.o\u001b[0m\r\n",
      "[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6.cpp.o\u001b[0m\r\n",
      "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6qwen2.cpp.o\u001b[0m\r\n",
      "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv7-base.cpp.o\u001b[0m\r\n",
      "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv7.cpp.o\u001b[0m\r\n",
      "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/seed-oss.cpp.o\u001b[0m\r\n",
      "[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/smallthinker.cpp.o\u001b[0m\r\n",
      "[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/smollm3.cpp.o\u001b[0m\r\n",
      "[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/stablelm.cpp.o\u001b[0m\r\n",
      "[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/starcoder.cpp.o\u001b[0m\r\n",
      "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/starcoder2.cpp.o\u001b[0m\r\n",
      "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/t5-dec.cpp.o\u001b[0m\r\n",
      "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/wavtokenizer-dec.cpp.o\u001b[0m\r\n",
      "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/t5-enc.cpp.o\u001b[0m\r\n",
      "[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/xverse.cpp.o\u001b[0m\r\n",
      "[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mistral3.cpp.o\u001b[0m\r\n",
      "[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/graph-context-mamba.cpp.o\u001b[0m\r\n",
      "[ 41%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf\u001b[0m\r\n",
      "[ 41%] Built target llama-gguf\r\n",
      "[ 42%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-hash\u001b[0m\r\n",
      "[ 42%] Built target llama-gguf-hash\r\n",
      "[ 42%] \u001b[32m\u001b[1mLinking CXX static library libcpp-httplib.a\u001b[0m\r\n",
      "[ 42%] Built target cpp-httplib\r\n",
      "[ 42%] \u001b[32m\u001b[1mLinking CXX shared library ../bin/libllama.so\u001b[0m\r\n",
      "[ 42%] Built target llama\r\n",
      "[ 42%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/arg.cpp.o\u001b[0m\r\n",
      "[ 42%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-parser-xml-toolcall.cpp.o\u001b[0m\r\n",
      "[ 42%] \u001b[32mBuilding C object tests/CMakeFiles/test-c.dir/test-c.c.o\u001b[0m\r\n",
      "[ 43%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd.cpp.o\u001b[0m\r\n",
      "[ 43%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-parser.cpp.o\u001b[0m\r\n",
      "[ 44%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-peg-parser.cpp.o\u001b[0m\r\n",
      "[ 44%] \u001b[32mBuilding CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o\u001b[0m\r\n",
      "[ 44%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-audio.cpp.o\u001b[0m\r\n",
      "[ 44%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-helper.cpp.o\u001b[0m\r\n",
      "[ 44%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/clip.cpp.o\u001b[0m\r\n",
      "[ 44%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat.cpp.o\u001b[0m\r\n",
      "[ 45%] \u001b[32mBuilding CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o\u001b[0m\r\n",
      "[ 46%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/cogvlm.cpp.o\u001b[0m\r\n",
      "[ 46%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/glm4v.cpp.o\u001b[0m\r\n",
      "[ 46%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\r\n",
      "[ 46%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/internvl.cpp.o\u001b[0m\r\n",
      "[ 46%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\r\n",
      "[ 46%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/kimivl.cpp.o\u001b[0m\r\n",
      "[ 47%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/download.cpp.o\u001b[0m\r\n",
      "[ 47%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-partial.cpp.o\u001b[0m\r\n",
      "[ 47%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\u001b[0m\r\n",
      "[ 47%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/llguidance.cpp.o\u001b[0m\r\n",
      "[ 48%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/llama4.cpp.o\u001b[0m\r\n",
      "[ 48%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/llava.cpp.o\u001b[0m\r\n",
      "[ 49%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/log.cpp.o\u001b[0m\r\n",
      "[ 49%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/minicpmv.cpp.o\u001b[0m\r\n",
      "[ 49%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/pixtral.cpp.o\u001b[0m\r\n",
      "[ 49%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\u001b[0m\r\n",
      "[ 49%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/peg-parser.cpp.o\u001b[0m\r\n",
      "[ 49%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/preset.cpp.o\u001b[0m\r\n",
      "[ 50%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/regex-partial.cpp.o\u001b[0m\r\n",
      "[ 51%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/qwen2vl.cpp.o\u001b[0m\r\n",
      "[ 51%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\r\n",
      "[ 51%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/qwen3vl.cpp.o\u001b[0m\r\n",
      "[ 51%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/siglip.cpp.o\u001b[0m\r\n",
      "[ 51%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/whisper-enc.cpp.o\u001b[0m\r\n",
      "[ 51%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/unicode.cpp.o\u001b[0m\r\n",
      "[ 51%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/speculative.cpp.o\u001b[0m\r\n",
      "[ 52%] \u001b[32m\u001b[1mLinking C executable ../bin/test-c\u001b[0m\r\n",
      "[ 52%] Built target test-c\r\n",
      "[ 52%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple\u001b[0m\r\n",
      "[ 52%] Built target llama-simple\r\n",
      "[ 52%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple-chat\u001b[0m\r\n",
      "[ 52%] Built target llama-simple-chat\r\n",
      "[ 53%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libmtmd.so\u001b[0m\r\n",
      "[ 53%] Built target mtmd\r\n",
      "[ 54%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\r\n",
      "[ 54%] Built target common\r\n",
      "[ 54%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\r\n",
      "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o\u001b[0m\r\n",
      "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o\u001b[0m\r\n",
      "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o\u001b[0m\r\n",
      "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o\u001b[0m\r\n",
      "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o\u001b[0m\r\n",
      "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o\u001b[0m\r\n",
      "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o\u001b[0m\r\n",
      "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o\u001b[0m\r\n",
      "[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o\u001b[0m\r\n",
      "[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-stats.dir/test-quantize-stats.cpp.o\u001b[0m\r\n",
      "[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o\u001b[0m\r\n",
      "[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o\u001b[0m\r\n",
      "[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gbnf-validator.dir/test-gbnf-validator.cpp.o\u001b[0m\r\n",
      "[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o\u001b[0m\r\n",
      "[ 58%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\r\n",
      "[ 58%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o\u001b[0m\r\n",
      "[ 59%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/test-chat-parser.cpp.o\u001b[0m\r\n",
      "[ 59%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-peg-parser.dir/test-chat-peg-parser.cpp.o\u001b[0m\r\n",
      "[ 59%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/get-model.cpp.o\u001b[0m\r\n",
      "[ 59%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o\u001b[0m\r\n",
      "[ 59%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-peg-parser.dir/get-model.cpp.o\u001b[0m\r\n",
      "[ 60%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-peg-parser.dir/peg-parser/simple-tokenize.cpp.o\u001b[0m\r\n",
      "[ 60%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o\u001b[0m\r\n",
      "[ 60%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/test-regex-partial.cpp.o\u001b[0m\r\n",
      "[ 60%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/test-json-partial.cpp.o\u001b[0m\r\n",
      "[ 60%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/get-model.cpp.o\u001b[0m\r\n",
      "[ 61%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o\u001b[0m\r\n",
      "[ 61%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/test-peg-parser.cpp.o\u001b[0m\r\n",
      "[ 61%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o\u001b[0m\r\n",
      "[ 61%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/test-thread-safety.cpp.o\u001b[0m\r\n",
      "[ 61%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o\u001b[0m\r\n",
      "[ 61%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/simple-tokenize.cpp.o\u001b[0m\r\n",
      "[ 61%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-opt.dir/test-opt.cpp.o\u001b[0m\r\n",
      "[ 62%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/get-model.cpp.o\u001b[0m\r\n",
      "[ 63%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o\u001b[0m\r\n",
      "[ 64%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/get-model.cpp.o\u001b[0m\r\n",
      "[ 65%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-basic.cpp.o\u001b[0m\r\n",
      "[ 66%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o\u001b[0m\r\n",
      "[ 67%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-opt.dir/get-model.cpp.o\u001b[0m\r\n",
      "[ 67%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-gbnf-generation.cpp.o\u001b[0m\r\n",
      "[ 67%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o\u001b[0m\r\n",
      "[ 67%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-json-parser.cpp.o\u001b[0m\r\n",
      "[ 67%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-json-serialization.cpp.o\u001b[0m\r\n",
      "[ 67%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o\u001b[0m\r\n",
      "[ 67%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o\u001b[0m\r\n",
      "[ 68%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-unicode.cpp.o\u001b[0m\r\n",
      "[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-state-restore-fragmented.dir/test-state-restore-fragmented.cpp.o\u001b[0m\r\n",
      "[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o\u001b[0m\r\n",
      "[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/get-model.cpp.o\u001b[0m\r\n",
      "[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o\u001b[0m\r\n",
      "[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o\u001b[0m\r\n",
      "[ 70%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o\u001b[0m\r\n",
      "[ 70%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-state-restore-fragmented.dir/get-model.cpp.o\u001b[0m\r\n",
      "[ 70%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o\u001b[0m\r\n",
      "[ 70%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\r\n",
      "[ 70%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o\u001b[0m\r\n",
      "[ 71%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o\u001b[0m\r\n",
      "[ 71%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\r\n",
      "[ 71%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o\u001b[0m\r\n",
      "[ 72%] \u001b[32mBuilding C object tests/CMakeFiles/test-mtmd-c-api.dir/test-mtmd-c-api.c.o\u001b[0m\r\n",
      "[ 73%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-alloc.dir/test-alloc.cpp.o\u001b[0m\r\n",
      "[ 73%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o\u001b[0m\r\n",
      "[ 73%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o\u001b[0m\r\n",
      "[ 74%] \u001b[32mBuilding CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o\u001b[0m\r\n",
      "[ 74%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-mtmd-c-api.dir/get-model.cpp.o\u001b[0m\r\n",
      "[ 74%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-alloc.dir/get-model.cpp.o\u001b[0m\r\n",
      "[ 75%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o\u001b[0m\r\n",
      "[ 75%] \u001b[32mBuilding CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o\u001b[0m\r\n",
      "[ 75%] \u001b[32mBuilding CXX object examples/idle/CMakeFiles/llama-idle.dir/idle.cpp.o\u001b[0m\r\n",
      "[ 75%] \u001b[32mBuilding CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o\u001b[0m\r\n",
      "[ 75%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o\u001b[0m\r\n",
      "[ 75%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o\u001b[0m\r\n",
      "[ 75%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o\u001b[0m\r\n",
      "[ 75%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o\u001b[0m\r\n",
      "[ 75%] \u001b[32mBuilding CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o\u001b[0m\r\n",
      "[ 75%] \u001b[32mBuilding CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o\u001b[0m\r\n",
      "[ 75%] \u001b[32mBuilding CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o\u001b[0m\r\n",
      "[ 75%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o\u001b[0m\r\n",
      "[ 75%] \u001b[32mBuilding CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o\u001b[0m\r\n",
      "[ 76%] \u001b[32mBuilding CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o\u001b[0m\r\n",
      "[ 76%] \u001b[32mBuilding CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o\u001b[0m\r\n",
      "[ 76%] \u001b[32mBuilding CXX object examples/training/CMakeFiles/llama-finetune.dir/finetune.cpp.o\u001b[0m\r\n",
      "[ 76%] \u001b[32mBuilding CXX object examples/diffusion/CMakeFiles/llama-diffusion-cli.dir/diffusion-cli.cpp.o\u001b[0m\r\n",
      "[ 76%] \u001b[32mBuilding CXX object examples/model-conversion/CMakeFiles/llama-logits.dir/logits.cpp.o\u001b[0m\r\n",
      "[ 76%] \u001b[32mBuilding CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\u001b[0m\r\n",
      "[ 76%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o\u001b[0m\r\n",
      "[ 76%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o\u001b[0m\r\n",
      "[ 76%] \u001b[32mBuilding CXX object tools/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o\u001b[0m\r\n",
      "[ 76%] \u001b[32mBuilding CXX object tools/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o\u001b[0m\r\n",
      "[ 76%] \u001b[32mBuilding CXX object tools/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o\u001b[0m\r\n",
      "[ 77%] \u001b[32mBuilding CXX object tools/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\u001b[0m\r\n",
      "[ 77%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/server-context.dir/server-task.cpp.o\u001b[0m\r\n",
      "[ 77%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/server-context.dir/server-queue.cpp.o\u001b[0m\r\n",
      "[ 77%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/server-context.dir/server-common.cpp.o\u001b[0m\r\n",
      "[ 78%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/server-context.dir/server-context.cpp.o\u001b[0m\r\n",
      "[ 79%] \u001b[32mBuilding CXX object tools/completion/CMakeFiles/llama-completion.dir/completion.cpp.o\u001b[0m\r\n",
      "[ 79%] \u001b[32mBuilding CXX object tools/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o\u001b[0m\r\n",
      "[ 79%] \u001b[32mBuilding CXX object tools/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\u001b[0m\r\n",
      "[ 79%] \u001b[32mBuilding CXX object tools/run/CMakeFiles/llama-run.dir/run.cpp.o\u001b[0m\r\n",
      "[ 79%] \u001b[32mBuilding CXX object tools/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o\u001b[0m\r\n",
      "[ 80%] \u001b[32mBuilding CXX object tools/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o\u001b[0m\r\n",
      "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-model-load-cancel\u001b[0m\r\n",
      "[ 81%] \u001b[32mBuilding CXX object tools/tts/CMakeFiles/llama-tts.dir/tts.cpp.o\u001b[0m\r\n",
      "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-mtmd-c-api\u001b[0m\r\n",
      "[ 81%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-mtmd-cli.dir/mtmd-cli.cpp.o\u001b[0m\r\n",
      "[ 82%] \u001b[32mBuilding CXX object tools/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o\u001b[0m\r\n",
      "[ 83%] \u001b[32mBuilding CXX object tools/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o\u001b[0m\r\n",
      "[ 84%] \u001b[32mBuilding CXX object tools/fit-params/CMakeFiles/llama-fit-params.dir/fit-params.cpp.o\u001b[0m\r\n",
      "[ 84%] Built target test-model-load-cancel\r\n",
      "[ 84%] Built target test-mtmd-c-api\r\n",
      "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-log\u001b[0m\r\n",
      "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-autorelease\u001b[0m\r\n",
      "[ 86%] Built target test-log\r\n",
      "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-rope\u001b[0m\r\n",
      "[ 87%] Built target test-autorelease\r\n",
      "[ 87%] Built target test-rope\r\n",
      "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-barrier\u001b[0m\r\n",
      "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\r\n",
      "[ 87%] Built target test-barrier\r\n",
      "[ 87%] Built target test-quantize-fns\r\n",
      "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-merge\u001b[0m\r\n",
      "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-q8dot\u001b[0m\r\n",
      "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-spm\u001b[0m\r\n",
      "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tokenize\u001b[0m\r\n",
      "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-bpe\u001b[0m\r\n",
      "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-vdot\u001b[0m\r\n",
      "[ 88%] Built target llama-lookup-merge\r\n",
      "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gbnf-validator\u001b[0m\r\n",
      "[ 88%] Built target llama-q8dot\r\n",
      "[ 88%] Built target test-tokenizer-1-spm\r\n",
      "[ 88%] Built target llama-tokenize\r\n",
      "[ 88%] Built target test-tokenizer-1-bpe\r\n",
      "[ 88%] Built target llama-vdot\r\n",
      "[ 88%] Built target test-gbnf-validator\r\n",
      "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-logits\u001b[0m\r\n",
      "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-alloc\u001b[0m\r\n",
      "[ 88%] Built target llama-logits\r\n",
      "[ 88%] Built target test-alloc\r\n",
      "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-fit-params\u001b[0m\r\n",
      "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-state-restore-fragmented\u001b[0m\r\n",
      "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-split\u001b[0m\r\n",
      "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-create\u001b[0m\r\n",
      "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-idle\u001b[0m\r\n",
      "[ 90%] Built target llama-gguf-split\r\n",
      "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0\u001b[0m\r\n",
      "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-finetune\u001b[0m\r\n",
      "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-stats\u001b[0m\r\n",
      "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gen-docs\u001b[0m\r\n",
      "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-save-load-state\u001b[0m\r\n",
      "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-sampling\u001b[0m\r\n",
      "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-parser\u001b[0m\r\n",
      "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-eval-callback\u001b[0m\r\n",
      "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative-simple\u001b[0m\r\n",
      "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-thread-safety\u001b[0m\r\n",
      "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched-bench\u001b[0m\r\n",
      "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched\u001b[0m\r\n",
      "[ 93%] Built target test-tokenizer-0\r\n",
      "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup\u001b[0m\r\n",
      "[ 93%] Built target test-sampling\r\n",
      "[ 93%] Built target test-grammar-parser\r\n",
      "[ 93%] Built target llama-fit-params\r\n",
      "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-llama-grammar\u001b[0m\r\n",
      "[ 93%] Built target test-state-restore-fragmented\r\n",
      "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-regex-partial\u001b[0m\r\n",
      "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\r\n",
      "[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-passkey\u001b[0m\r\n",
      "[ 95%] Built target llama-lookup-create\r\n",
      "[ 95%] Built target llama-idle\r\n",
      "[ 95%] Built target test-llama-grammar\r\n",
      "[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-arg-parser\u001b[0m\r\n",
      "[ 95%] Built target llama-finetune\r\n",
      "[ 95%] Built target llama-lookup-stats\r\n",
      "[ 95%] Built target test-regex-partial\r\n",
      "[ 95%] Built target llama-save-load-state\r\n",
      "[ 95%] Built target test-quantize-perf\r\n",
      "[ 95%] Built target llama-gen-docs\r\n",
      "[ 95%] Built target test-thread-safety\r\n",
      "[ 95%] Built target llama-eval-callback\r\n",
      "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookahead\u001b[0m\r\n",
      "[ 96%] Built target llama-batched-bench\r\n",
      "[ 96%] Built target llama-speculative-simple\r\n",
      "[ 96%] Built target llama-batched\r\n",
      "[ 96%] Built target llama-lookup\r\n",
      "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-opt\u001b[0m\r\n",
      "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-embedding\u001b[0m\r\n",
      "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-convert-llama2c-to-ggml\u001b[0m\r\n",
      "[ 96%] Built target test-opt\r\n",
      "[ 96%] Built target llama-passkey\r\n",
      "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-parallel\u001b[0m\r\n",
      "[ 96%] Built target test-arg-parser\r\n",
      "[ 96%] Built target llama-convert-llama2c-to-ggml\r\n",
      "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gguf\u001b[0m\r\n",
      "[ 96%] Built target llama-lookahead\r\n",
      "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-retrieval\u001b[0m\r\n",
      "[ 96%] Built target test-gguf\r\n",
      "[ 96%] Built target llama-embedding\r\n",
      "[ 96%] Built target llama-parallel\r\n",
      "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-export-lora\u001b[0m\r\n",
      "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize\u001b[0m\r\n",
      "[ 96%] Built target llama-quantize\r\n",
      "[ 96%] Built target llama-retrieval\r\n",
      "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-mtmd-cli\u001b[0m\r\n",
      "[ 97%] Built target llama-export-lora\r\n",
      "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cvector-generator\u001b[0m\r\n",
      "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative\u001b[0m\r\n",
      "[ 97%] Built target llama-mtmd-cli\r\n",
      "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-diffusion-cli\u001b[0m\r\n",
      "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-template\u001b[0m\r\n",
      "[ 97%] Built target llama-cvector-generator\r\n",
      "[ 97%] Built target test-chat-template\r\n",
      "[ 97%] Built target llama-speculative\r\n",
      "[ 97%] Built target llama-diffusion-cli\r\n",
      "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-partial\u001b[0m\r\n",
      "[ 97%] Built target test-json-partial\r\n",
      "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-completion\u001b[0m\r\n",
      "[ 97%] Built target llama-completion\r\n",
      "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-stats\u001b[0m\r\n",
      "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-perplexity\u001b[0m\r\n",
      "[ 97%] Built target test-quantize-stats\r\n",
      "[ 97%] Built target llama-perplexity\r\n",
      "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-run\u001b[0m\r\n",
      "[ 97%] Built target llama-run\r\n",
      "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-integration\u001b[0m\r\n",
      "[ 97%] Built target test-grammar-integration\r\n",
      "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-peg-parser\u001b[0m\r\n",
      "[ 97%] Built target test-peg-parser\r\n",
      "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-parser\u001b[0m\r\n",
      "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-imatrix\u001b[0m\r\n",
      "[ 97%] Built target test-chat-parser\r\n",
      "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-schema-to-grammar\u001b[0m\r\n",
      "[ 97%] Built target test-json-schema-to-grammar\r\n",
      "[ 97%] Built target llama-imatrix\r\n",
      "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tts\u001b[0m\r\n",
      "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-bench\u001b[0m\r\n",
      "[ 97%] Built target llama-bench\r\n",
      "[ 97%] Built target llama-tts\r\n",
      "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-peg-parser\u001b[0m\r\n",
      "[ 97%] Built target test-chat-peg-parser\r\n",
      "[ 97%] \u001b[32m\u001b[1mLinking CXX static library libserver-context.a\u001b[0m\r\n",
      "[ 97%] Built target server-context\r\n",
      "[ 97%] \u001b[34m\u001b[1mGenerating loading.html.hpp\u001b[0m\r\n",
      "[ 98%] \u001b[34m\u001b[1mGenerating index.html.gz.hpp\u001b[0m\r\n",
      "[ 98%] \u001b[32mBuilding CXX object tools/cli/CMakeFiles/llama-cli.dir/cli.cpp.o\u001b[0m\r\n",
      "[ 98%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-http.cpp.o\u001b[0m\r\n",
      "[ 98%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server.cpp.o\u001b[0m\r\n",
      "[ 99%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-models.cpp.o\u001b[0m\r\n",
      "[ 99%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-task.cpp.o\u001b[0m\r\n",
      "[ 99%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-queue.cpp.o\u001b[0m\r\n",
      "[ 99%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-common.cpp.o\u001b[0m\r\n",
      "[100%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-context.cpp.o\u001b[0m\r\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-ops\u001b[0m\r\n",
      "[100%] Built target test-backend-ops\r\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cli\u001b[0m\r\n",
      "[100%] Built target llama-cli\r\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat\u001b[0m\r\n",
      "[100%] Built target test-chat\r\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-server\u001b[0m\r\n",
      "[100%] Built target llama-server\r\n"
     ]
    }
   ],
   "source": [
    "!cd llama.cpp && mkdir -p build && cd build && cmake .. && make -j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04dc9268",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T16:39:08.185451Z",
     "iopub.status.busy": "2025-12-16T16:39:08.185160Z",
     "iopub.status.idle": "2025-12-16T16:39:13.210156Z",
     "shell.execute_reply": "2025-12-16T16:39:13.209143Z"
    },
    "papermill": {
     "duration": 5.076738,
     "end_time": "2025-12-16T16:39:13.211710",
     "exception": false,
     "start_time": "2025-12-16T16:39:08.134972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 7437 (ec98e2002)\r\n",
      "main: built with GNU 11.4.0 for Linux x86_64\r\n",
      "main: quantizing '/kaggle/working/chess-fp16.gguf' to '/kaggle/working/chess-q8_0.gguf' as Q8_0\r\n",
      "llama_model_loader: loaded meta data with 29 key-value pairs and 310 tensors from /kaggle/working/chess-fp16.gguf (version GGUF V3 (latest))\r\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen3\r\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\r\n",
      "llama_model_loader: - kv   2:                     general.sampling.top_k i32              = 20\r\n",
      "llama_model_loader: - kv   3:                     general.sampling.top_p f32              = 0.950000\r\n",
      "llama_model_loader: - kv   4:                      general.sampling.temp f32              = 0.600000\r\n",
      "llama_model_loader: - kv   5:                               general.name str              = Chess Qwen Adapted\r\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 596M\r\n",
      "llama_model_loader: - kv   7:                          qwen3.block_count u32              = 28\r\n",
      "llama_model_loader: - kv   8:                       qwen3.context_length u32              = 40960\r\n",
      "llama_model_loader: - kv   9:                     qwen3.embedding_length u32              = 1024\r\n",
      "llama_model_loader: - kv  10:                  qwen3.feed_forward_length u32              = 3072\r\n",
      "llama_model_loader: - kv  11:                 qwen3.attention.head_count u32              = 16\r\n",
      "llama_model_loader: - kv  12:              qwen3.attention.head_count_kv u32              = 8\r\n",
      "llama_model_loader: - kv  13:                       qwen3.rope.freq_base f32              = 1000000.000000\r\n",
      "llama_model_loader: - kv  14:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\r\n",
      "llama_model_loader: - kv  15:                 qwen3.attention.key_length u32              = 128\r\n",
      "llama_model_loader: - kv  16:               qwen3.attention.value_length u32              = 128\r\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 1\r\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\r\n",
      "llama_model_loader: - kv  19:                       tokenizer.ggml.model str              = gpt2\r\n",
      "llama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = qwen2\r\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\r\n",
      "llama_model_loader: - kv  22:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\n",
      "llama_model_loader: - kv  23:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ä  Ä \", \"Ä Ä  Ä Ä \", \"i n\", \"Ä  t\",...\r\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 151645\r\n",
      "llama_model_loader: - kv  25:            tokenizer.ggml.padding_token_id u32              = 151645\r\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 151643\r\n",
      "llama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = false\r\n",
      "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\r\n",
      "llama_model_loader: - type  f32:  113 tensors\r\n",
      "llama_model_loader: - type  f16:  197 tensors\r\n",
      "[   1/ 310]                   output_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[   2/ 310]                    token_embd.weight - [ 1024, 151936,     1,     1], type =    f16, converting to q8_0 .. size =   296.75 MiB ->   157.65 MiB\r\n",
      "[   3/ 310]                  blk.0.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[   4/ 310]             blk.0.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[   5/ 310]               blk.0.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[   6/ 310]             blk.0.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[   7/ 310]                  blk.0.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[   8/ 310]             blk.0.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[   9/ 310]                  blk.0.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[  10/ 310]                blk.0.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[  11/ 310]                blk.0.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[  12/ 310]                blk.0.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[  13/ 310]                  blk.0.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[  14/ 310]                  blk.1.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[  15/ 310]             blk.1.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[  16/ 310]               blk.1.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[  17/ 310]             blk.1.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[  18/ 310]                  blk.1.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[  19/ 310]             blk.1.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[  20/ 310]                  blk.1.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[  21/ 310]                blk.1.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[  22/ 310]                blk.1.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[  23/ 310]                blk.1.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[  24/ 310]                  blk.1.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[  25/ 310]                  blk.2.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[  26/ 310]             blk.2.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[  27/ 310]               blk.2.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[  28/ 310]             blk.2.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[  29/ 310]                  blk.2.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[  30/ 310]             blk.2.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[  31/ 310]                  blk.2.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[  32/ 310]                blk.2.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[  33/ 310]                blk.2.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[  34/ 310]                blk.2.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[  35/ 310]                  blk.2.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[  36/ 310]                  blk.3.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[  37/ 310]             blk.3.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[  38/ 310]               blk.3.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[  39/ 310]             blk.3.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[  40/ 310]                  blk.3.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[  41/ 310]             blk.3.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[  42/ 310]                  blk.3.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[  43/ 310]                blk.3.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[  44/ 310]                blk.3.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[  45/ 310]                blk.3.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[  46/ 310]                  blk.3.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[  47/ 310]                  blk.4.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[  48/ 310]             blk.4.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[  49/ 310]               blk.4.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[  50/ 310]             blk.4.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[  51/ 310]                  blk.4.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[  52/ 310]             blk.4.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[  53/ 310]                  blk.4.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[  54/ 310]                blk.4.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[  55/ 310]                blk.4.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[  56/ 310]                blk.4.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[  57/ 310]                  blk.4.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[  58/ 310]                  blk.5.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[  59/ 310]             blk.5.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[  60/ 310]               blk.5.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[  61/ 310]             blk.5.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[  62/ 310]                  blk.5.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[  63/ 310]             blk.5.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[  64/ 310]                  blk.5.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[  65/ 310]                blk.5.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[  66/ 310]                blk.5.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[  67/ 310]                blk.5.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[  68/ 310]                  blk.5.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[  69/ 310]                  blk.6.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[  70/ 310]             blk.6.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[  71/ 310]               blk.6.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[  72/ 310]             blk.6.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[  73/ 310]                  blk.6.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[  74/ 310]             blk.6.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[  75/ 310]                  blk.6.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[  76/ 310]                blk.6.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[  77/ 310]                blk.6.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[  78/ 310]                blk.6.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[  79/ 310]                  blk.6.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[  80/ 310]                  blk.7.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[  81/ 310]             blk.7.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[  82/ 310]               blk.7.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[  83/ 310]             blk.7.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[  84/ 310]                  blk.7.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[  85/ 310]             blk.7.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[  86/ 310]                  blk.7.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[  87/ 310]                blk.7.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[  88/ 310]                blk.7.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[  89/ 310]                blk.7.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[  90/ 310]                  blk.7.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[  91/ 310]                  blk.8.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[  92/ 310]             blk.8.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[  93/ 310]               blk.8.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[  94/ 310]             blk.8.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[  95/ 310]                  blk.8.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[  96/ 310]             blk.8.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[  97/ 310]                  blk.8.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[  98/ 310]                blk.8.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[  99/ 310]                blk.8.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 100/ 310]                blk.8.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[ 101/ 310]                  blk.8.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 102/ 310]                  blk.9.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[ 103/ 310]             blk.9.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[ 104/ 310]               blk.9.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[ 105/ 310]             blk.9.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[ 106/ 310]                  blk.9.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[ 107/ 310]             blk.9.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[ 108/ 310]                  blk.9.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[ 109/ 310]                blk.9.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 110/ 310]                blk.9.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 111/ 310]                blk.9.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[ 112/ 310]                  blk.9.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 113/ 310]                 blk.10.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[ 114/ 310]            blk.10.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[ 115/ 310]              blk.10.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[ 116/ 310]            blk.10.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[ 117/ 310]                 blk.10.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[ 118/ 310]            blk.10.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[ 119/ 310]                 blk.10.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[ 120/ 310]               blk.10.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 121/ 310]               blk.10.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 122/ 310]               blk.10.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[ 123/ 310]                 blk.10.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 124/ 310]                 blk.11.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[ 125/ 310]            blk.11.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[ 126/ 310]              blk.11.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[ 127/ 310]            blk.11.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[ 128/ 310]                 blk.11.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[ 129/ 310]            blk.11.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[ 130/ 310]                 blk.11.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[ 131/ 310]               blk.11.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 132/ 310]               blk.11.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 133/ 310]               blk.11.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[ 134/ 310]                 blk.11.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 135/ 310]                 blk.12.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[ 136/ 310]            blk.12.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[ 137/ 310]              blk.12.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[ 138/ 310]            blk.12.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[ 139/ 310]                 blk.12.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[ 140/ 310]            blk.12.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[ 141/ 310]                 blk.12.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[ 142/ 310]               blk.12.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 143/ 310]               blk.12.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 144/ 310]               blk.12.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[ 145/ 310]                 blk.12.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 146/ 310]                 blk.13.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[ 147/ 310]            blk.13.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[ 148/ 310]              blk.13.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[ 149/ 310]            blk.13.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[ 150/ 310]                 blk.13.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[ 151/ 310]            blk.13.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[ 152/ 310]                 blk.13.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[ 153/ 310]               blk.13.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 154/ 310]               blk.13.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 155/ 310]               blk.13.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[ 156/ 310]                 blk.13.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 157/ 310]                 blk.14.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[ 158/ 310]            blk.14.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[ 159/ 310]              blk.14.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[ 160/ 310]            blk.14.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[ 161/ 310]                 blk.14.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[ 162/ 310]            blk.14.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[ 163/ 310]                 blk.14.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[ 164/ 310]               blk.14.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 165/ 310]               blk.14.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 166/ 310]               blk.14.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[ 167/ 310]                 blk.14.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 168/ 310]                 blk.15.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[ 169/ 310]            blk.15.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[ 170/ 310]              blk.15.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[ 171/ 310]            blk.15.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[ 172/ 310]                 blk.15.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[ 173/ 310]            blk.15.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[ 174/ 310]                 blk.15.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[ 175/ 310]               blk.15.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 176/ 310]               blk.15.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 177/ 310]               blk.15.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[ 178/ 310]                 blk.15.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 179/ 310]                 blk.16.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[ 180/ 310]            blk.16.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[ 181/ 310]              blk.16.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[ 182/ 310]            blk.16.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[ 183/ 310]                 blk.16.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[ 184/ 310]            blk.16.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[ 185/ 310]                 blk.16.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[ 186/ 310]               blk.16.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 187/ 310]               blk.16.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 188/ 310]               blk.16.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[ 189/ 310]                 blk.16.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 190/ 310]                 blk.17.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[ 191/ 310]            blk.17.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[ 192/ 310]              blk.17.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[ 193/ 310]            blk.17.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[ 194/ 310]                 blk.17.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[ 195/ 310]            blk.17.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[ 196/ 310]                 blk.17.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[ 197/ 310]               blk.17.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 198/ 310]               blk.17.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 199/ 310]               blk.17.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[ 200/ 310]                 blk.17.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 201/ 310]                 blk.18.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[ 202/ 310]            blk.18.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[ 203/ 310]              blk.18.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[ 204/ 310]            blk.18.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[ 205/ 310]                 blk.18.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[ 206/ 310]            blk.18.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[ 207/ 310]                 blk.18.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[ 208/ 310]               blk.18.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 209/ 310]               blk.18.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 210/ 310]               blk.18.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[ 211/ 310]                 blk.18.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 212/ 310]                 blk.19.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[ 213/ 310]            blk.19.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[ 214/ 310]              blk.19.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[ 215/ 310]            blk.19.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[ 216/ 310]                 blk.19.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[ 217/ 310]            blk.19.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[ 218/ 310]                 blk.19.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[ 219/ 310]               blk.19.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 220/ 310]               blk.19.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 221/ 310]               blk.19.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[ 222/ 310]                 blk.19.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 223/ 310]                 blk.20.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[ 224/ 310]            blk.20.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[ 225/ 310]              blk.20.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[ 226/ 310]            blk.20.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[ 227/ 310]                 blk.20.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[ 228/ 310]            blk.20.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[ 229/ 310]                 blk.20.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[ 230/ 310]               blk.20.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 231/ 310]               blk.20.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 232/ 310]               blk.20.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[ 233/ 310]                 blk.20.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 234/ 310]                 blk.21.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[ 235/ 310]            blk.21.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[ 236/ 310]              blk.21.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[ 237/ 310]            blk.21.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[ 238/ 310]                 blk.21.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[ 239/ 310]            blk.21.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[ 240/ 310]                 blk.21.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[ 241/ 310]               blk.21.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 242/ 310]               blk.21.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 243/ 310]               blk.21.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[ 244/ 310]                 blk.21.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 245/ 310]                 blk.22.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[ 246/ 310]            blk.22.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[ 247/ 310]              blk.22.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[ 248/ 310]            blk.22.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[ 249/ 310]                 blk.22.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[ 250/ 310]            blk.22.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[ 251/ 310]                 blk.22.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[ 252/ 310]               blk.22.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 253/ 310]               blk.22.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 254/ 310]               blk.22.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[ 255/ 310]                 blk.22.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 256/ 310]                 blk.23.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[ 257/ 310]            blk.23.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[ 258/ 310]              blk.23.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[ 259/ 310]            blk.23.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[ 260/ 310]                 blk.23.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[ 261/ 310]            blk.23.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[ 262/ 310]                 blk.23.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[ 263/ 310]               blk.23.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 264/ 310]               blk.23.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 265/ 310]               blk.23.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[ 266/ 310]                 blk.23.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 267/ 310]                 blk.24.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[ 268/ 310]            blk.24.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[ 269/ 310]              blk.24.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[ 270/ 310]            blk.24.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[ 271/ 310]                 blk.24.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[ 272/ 310]            blk.24.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[ 273/ 310]                 blk.24.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[ 274/ 310]               blk.24.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 275/ 310]               blk.24.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 276/ 310]               blk.24.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[ 277/ 310]                 blk.24.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 278/ 310]                 blk.25.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[ 279/ 310]            blk.25.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[ 280/ 310]              blk.25.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[ 281/ 310]            blk.25.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[ 282/ 310]                 blk.25.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[ 283/ 310]            blk.25.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[ 284/ 310]                 blk.25.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[ 285/ 310]               blk.25.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 286/ 310]               blk.25.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 287/ 310]               blk.25.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[ 288/ 310]                 blk.25.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 289/ 310]                 blk.26.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[ 290/ 310]            blk.26.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[ 291/ 310]              blk.26.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[ 292/ 310]            blk.26.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[ 293/ 310]                 blk.26.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[ 294/ 310]            blk.26.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[ 295/ 310]                 blk.26.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[ 296/ 310]               blk.26.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 297/ 310]               blk.26.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 298/ 310]               blk.26.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[ 299/ 310]                 blk.26.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 300/ 310]                 blk.27.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[ 301/ 310]            blk.27.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[ 302/ 310]              blk.27.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[ 303/ 310]            blk.27.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[ 304/ 310]                 blk.27.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     4.00 MiB ->     2.12 MiB\r\n",
      "[ 305/ 310]            blk.27.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\r\n",
      "[ 306/ 310]                 blk.27.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     2.00 MiB ->     1.06 MiB\r\n",
      "[ 307/ 310]               blk.27.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 308/ 310]               blk.27.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "[ 309/ 310]               blk.27.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\r\n",
      "[ 310/ 310]                 blk.27.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\r\n",
      "llama_model_quantize_impl: model size  =  1137.00 MiB\r\n",
      "llama_model_quantize_impl: quant size  =   604.15 MiB\r\n",
      "\r\n",
      "main: quantize time =  4787.12 ms\r\n",
      "main:    total time =  4787.12 ms\r\n"
     ]
    }
   ],
   "source": [
    "!llama.cpp/build/bin/llama-quantize \\\n",
    "    /kaggle/working/chess-fp16.gguf \\\n",
    "    /kaggle/working/chess-q8_0.gguf \\\n",
    "    q8_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb8bc67",
   "metadata": {
    "papermill": {
     "duration": 0.053493,
     "end_time": "2025-12-16T16:39:13.320025",
     "exception": false,
     "start_time": "2025-12-16T16:39:13.266532",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# compare quantize model and two pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa88c03c",
   "metadata": {
    "papermill": {
     "duration": 0.053697,
     "end_time": "2025-12-16T16:39:13.427517",
     "exception": false,
     "start_time": "2025-12-16T16:39:13.373820",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## define the funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67b6c29f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T16:39:13.536254Z",
     "iopub.status.busy": "2025-12-16T16:39:13.535360Z",
     "iopub.status.idle": "2025-12-16T16:39:13.540297Z",
     "shell.execute_reply": "2025-12-16T16:39:13.539734Z"
    },
    "papermill": {
     "duration": 0.06094,
     "end_time": "2025-12-16T16:39:13.541580",
     "exception": false,
     "start_time": "2025-12-16T16:39:13.480640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "position_analyze_prompt = \"\"\"\n",
    "You are a chess analysis engine.\n",
    "\n",
    "All evaluations are from White's perspective.\n",
    "Positive means advantage for White.\n",
    "\n",
    "Given Stockfish candidate lines, evaluate the position Who is better and why\n",
    "Rules:\n",
    "- Write only 2 or 3 sentences.\n",
    "- Do NOT mention move names.\n",
    "- Do NOT mention numbers, evaluations, or scores.\n",
    "- Use plain natural language.\n",
    "\n",
    "Position analysis:\n",
    "{analysis}\n",
    "\"\"\"\n",
    "\n",
    "def llm_position_analysis(stockfish_text: str,llm_func) -> str:\n",
    "    prompt = position_analyze_prompt.format(\n",
    "        analysis=stockfish_text\n",
    "    )\n",
    "    return llm_func(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ded94b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T16:39:13.650196Z",
     "iopub.status.busy": "2025-12-16T16:39:13.649898Z",
     "iopub.status.idle": "2025-12-16T16:39:13.654528Z",
     "shell.execute_reply": "2025-12-16T16:39:13.653966Z"
    },
    "papermill": {
     "duration": 0.060663,
     "end_time": "2025-12-16T16:39:13.655835",
     "exception": false,
     "start_time": "2025-12-16T16:39:13.595172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_analyze_prompt = \"\"\"\n",
    "You are a chess position evaluator.\n",
    "\n",
    "All evaluations are from Whiteâ€™s perspective.\n",
    "\n",
    "Based on the position before and after the move:\n",
    "- Decide if the move is good or bad.\n",
    "- If good, explain why it improves the position.\n",
    "- If bad, explain why it weakens the position.\n",
    "- If the change is small, briefly describe the position.\n",
    "\n",
    "Rules:\n",
    "- Write only 2 or 3 sentences.\n",
    "- Do NOT mention move names.\n",
    "- Do NOT mention numbers, evaluations, or scores.\n",
    "- Use plain natural language.\n",
    "\n",
    "Before position analysis:\n",
    "{before_llm}\n",
    "\n",
    "After position analysis:\n",
    "{after_llm}\n",
    "\n",
    "\"\"\"\n",
    "def llm_move_analysis(before_llm, after_llm, eval_before, eval_after,llm_func):\n",
    "    prompt = final_analyze_prompt.format(\n",
    "        before_llm=before_llm,\n",
    "        after_llm=after_llm,\n",
    "        eval_before=eval_before,\n",
    "        eval_after=eval_after,\n",
    "    )\n",
    "    return llm_func(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d752b176",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T16:39:13.762695Z",
     "iopub.status.busy": "2025-12-16T16:39:13.762433Z",
     "iopub.status.idle": "2025-12-16T16:39:13.767453Z",
     "shell.execute_reply": "2025-12-16T16:39:13.766915Z"
    },
    "papermill": {
     "duration": 0.059539,
     "end_time": "2025-12-16T16:39:13.768455",
     "exception": false,
     "start_time": "2025-12-16T16:39:13.708916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def full_llm_pipeline(sample,llm_func):\n",
    "    # Step 1: LLM evaluates positions independently\n",
    "    before_llm = llm_position_analysis(sample[\"before\"],llm_func)\n",
    "    after_llm  = llm_position_analysis(sample[\"after\"],llm_func)\n",
    "\n",
    "    # Step 2: LLM compares its OWN outputs + Stockfish eval\n",
    "    final_eval = llm_move_analysis(\n",
    "        before_llm,\n",
    "        after_llm,\n",
    "        sample['before'][\"eval\"],\n",
    "        sample['after'][\"eval\"],\n",
    "        llm_func\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"before_llm\": before_llm,\n",
    "        \"after_llm\": after_llm,\n",
    "        \"final_analysis\": final_eval\n",
    "    }\n",
    "    \n",
    "def simple_method(sample,llm_func):\n",
    "    prompt = f\"\"\"\n",
    "Analyze the following move and position.\n",
    "Rules:\n",
    "- Write only 2 or 3 sentences.\n",
    "- Do NOT mention move names.\n",
    "- Do NOT mention numbers, evaluations, or scores.\n",
    "- Use plain natural language.\n",
    "\n",
    "{sample[\"before\"]}\n",
    "{sample[\"after\"]}\n",
    "\n",
    "Eval before: {sample['before'][\"eval\"]}\n",
    "Eval after: {sample['after'][\"eval\"]}\n",
    "\"\"\"\n",
    "    return llm_func(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f149bc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T16:39:13.875691Z",
     "iopub.status.busy": "2025-12-16T16:39:13.875066Z",
     "iopub.status.idle": "2025-12-16T16:39:14.628503Z",
     "shell.execute_reply": "2025-12-16T16:39:14.627869Z"
    },
    "papermill": {
     "duration": 0.808815,
     "end_time": "2025-12-16T16:39:14.629880",
     "exception": false,
     "start_time": "2025-12-16T16:39:13.821065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "gguf_llm = Llama(\n",
    "    model_path=\"/kaggle/working/chess-q8_0.gguf\",\n",
    "    n_ctx=4096,\n",
    "    n_threads=8,\n",
    "    temperature=0.35,\n",
    "    top_p=0.9,\n",
    "    repeat_penalty=1.15,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "def chat_gguf(prompt: str, max_tokens: int = 64) -> str:\n",
    "    \"\"\"\n",
    "    Qwen-style GGUF chat, faithful to llama-cli usage\n",
    "    \"\"\"\n",
    "\n",
    "    full_prompt = (\n",
    "        \"<|im_start|>user\\n\"\n",
    "        \">> \" + prompt.strip() + \"\\n\"\n",
    "        \">> <|im_end|>\\n\"\n",
    "        \">> <|im_start|>assistant\"\n",
    "    )\n",
    "\n",
    "    output = gguf_llm(\n",
    "        full_prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        stop=[\"<|im_end|>\"],   # reverse-prompt equivalent\n",
    "    )\n",
    "\n",
    "    text = output[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "    # Safety cleanup: stop at first sentence end if needed\n",
    "    for end in [\".\", \"?\", \"!\"]:\n",
    "        if end in text:\n",
    "            text = text[:text.rfind(end) + 1]\n",
    "            break\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a92e48a",
   "metadata": {
    "papermill": {
     "duration": 0.053513,
     "end_time": "2025-12-16T16:39:14.737582",
     "exception": false,
     "start_time": "2025-12-16T16:39:14.684069",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## load the sample data with stockfish labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6dda6ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T16:39:14.845327Z",
     "iopub.status.busy": "2025-12-16T16:39:14.844831Z",
     "iopub.status.idle": "2025-12-16T16:39:14.854430Z",
     "shell.execute_reply": "2025-12-16T16:39:14.853869Z"
    },
    "papermill": {
     "duration": 0.065173,
     "end_time": "2025-12-16T16:39:14.855546",
     "exception": false,
     "start_time": "2025-12-16T16:39:14.790373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "samples = [\n",
    "  {\n",
    "    \"move_type\": \"blunder\",\n",
    "    \"move_number\": 1,\n",
    "    \"before\": {\n",
    "      \"fen\": \"r2qk2r/ppp2ppp/2n5/3n4/Q1B5/P1P1BP2/5P1P/R3K2R w KQkq - 0 14\",\n",
    "      \"stockfish_analysis\": \"[White POV]: 86.8% cp:376 O-O-O Qf6 Rxd5 O-O Qc2 Rae8 Bb3 Qxf3 Rhd1 Qh3 a4 Qxh2 Kb2 g6 (Depth: 20)\\n[White POV]: 85.4% cp:354 Rd1 Nxc3 Rxd8+ Rxd8 Qb3 Rd1+ Qxd1 Nxd1 Kxd1 Ke7 Bg5+ f6 Re1+ Kd7 Be3 Rd8 Kc1 g6 h4 Ne5 (Depth: 20)\\n[White POV]: 30.8% cp:-162 Qb3 Nxe3 Bxf7+ Kf8 fxe3 Qe7 O-O-O Qxf7 Qxb7 Qe8 Qxc7 h5 Rhg1 Qxe3+ Kc2 Ne7 Rge1 Qf2+ Kc1 Re8 (Depth: 20)\",\n",
    "      \"eval\": \"86.76111264579346\"\n",
    "    },\n",
    "    \"after\": {\n",
    "      \"fen\": \"r2qk2r/ppp2ppp/2n5/3B4/Q7/P1P1BP2/5P1P/R3K2R b KQkq - 0 14\",\n",
    "      \"stockfish_analysis\": \"[White POV]: 21.4% cp:-260 Qxd5 Qe4+ Qxe4 fxe4 O-O-O Ke2 g6 f3 Rhe8 h4 f5 exf5 gxf5 Kf2 Ne5 Bg5 Nd3+ Kf1 Rd7 Rd1 (Depth: 19)\\n[White POV]: 90.3% cp:446 O-O Bxc6 Qf6 Bxb7 Rab8 Qd4 Qg6 Be4 c5 Qd2 Qg2 Ke2 f5 Bd5+ Kh8 Rhg1 (Depth: 19)\\n[White POV]: 92.2% cp:493 Qd6 Bxc6+ bxc6 Qe4+ Qe6 Qxe6+ fxe6 f4 O-O-O h4 Rhf8 h5 Rf5 Ke2 Kb7 Rab1+ Ka8 c4 (Depth: 19)\",\n",
    "      \"eval\": \"92.16514750369404\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"move_type\": \"normal\",\n",
    "    \"move_number\": 1,\n",
    "    \"before\": {\n",
    "      \"fen\": \"rnbq1rk1/p1p1bppp/1p2pn2/3p4/2PP1B2/2N1PN2/PP3PPP/R2QKB1R w KQ - 0 7\",\n",
    "      \"stockfish_analysis\": \"[White POV]: 54.0% cp:32 cxd5 Nxd5 Nxd5 exd5 Be2 Bb4+ Nd2 Be7 O-O c5 Nf3 Nc6 b3 a6 dxc5 bxc5 Ne5 Nxe5 Bxe5 Be6 (Depth: 21)\\n[White POV]: 53.6% cp:29 Be2 dxc4 Bxc4 Bb7 O-O a6 Qe2 b5 Bd3 c5 dxc5 Bxc5 e4 Nbd7 Rad1 Nh5 Be3 Qc7 (Depth: 21)\\n[White POV]: 52.6% cp:21 Rc1 c5 dxc5 bxc5 Be2 Bb7 O-O Nbd7 cxd5 Nxd5 Nxd5 exd5 Qb3 Qb6 Rfd1 Nf6 Ne5 Bd6 Qxb6 axb6 (Depth: 21)\",\n",
    "      \"eval\": \"53.99148845555657\"\n",
    "    },\n",
    "    \"after\": {\n",
    "      \"fen\": \"rnbq1rk1/p1p1bppp/1p2pn2/3p4/2PP1B2/2NBPN2/PP3PPP/R2QK2R b KQ - 1 7\",\n",
    "      \"stockfish_analysis\": \"[White POV]: 53.7% cp:30 dxc4 Bxc4 Bb7 O-O Nbd7 Qe2 a6 a4 c5 Rad1 cxd4 exd4 Re8 Ne5 Bb4 Rfe1 Nxe5 dxe5 Nd5 Qg4 (Depth: 20)\\n[White POV]: 54.9% cp:39 Ba6 cxd5 Nxd5 Nxd5 Qxd5 O-O Qb7 Rc1 Bxd3 Qxd3 Rc8 Ne5 f6 Nf3 c5 Rfd1 Nd7 (Depth: 20)\\n[White POV]: 55.2% cp:42 c5 cxd5 Nxd5 Nxd5 exd5 Ne5 Ba6 dxc5 bxc5 O-O Bxd3 Qxd3 c4 Qe2 f6 Nf3 Nc6 b3 c3 Rac1 (Depth: 20)\",\n",
    "      \"eval\": \"55.230790957432525\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"move_type\": \"inaccuracy\",\n",
    "    \"move_number\": 1,\n",
    "    \"before\": {\n",
    "      \"fen\": \"rnb1kb1r/pp3ppp/1qp2p2/8/3P4/8/PPPB1PPP/R2QKBNR w KQkq - 0 8\",\n",
    "      \"stockfish_analysis\": \"[White POV]: 55.5% cp:44 Nf3 Be6 a3 g6 c4 Bg7 Be2 O-O O-O Re8 Re1 Nd7 d5 cxd5 Be3 Qd8 cxd5 Bg4 d6 (Depth: 18)\\n[White POV]: 54.6% cp:37 c3 Bf5 Bc4 Qxb2 Nf3 Qc2 Qe2+ Qe4 Qxe4+ Bxe4 O-O b5 Bb3 Kd7 a4 Bd6 axb5 cxb5 Rfe1 (Depth: 18)\\n[White POV]: 52.6% cp:21 Bd3 Qxd4 Nf3 Qd5 O-O Bd6 Qe1+ Be6 Bb4 Bxb4 Qxb4 c5 Qa4+ Nd7 Be4 Qc4 Qxc4 Bxc4 (Depth: 18)\",\n",
    "      \"eval\": \"55.477923510721475\"\n",
    "    },\n",
    "    \"after\": {\n",
    "      \"fen\": \"rnb1kb1r/pp3ppp/1qp2p2/8/3P4/1P6/P1PB1PPP/R2QKBNR b KQkq - 0 8\",\n",
    "      \"stockfish_analysis\": \"[White POV]: 43.4% cp:-53 Qxd4 Nf3 Qd8 Bd3 Bc5 Qe2+ Qe7 Qxe7+ Bxe7 O-O O-O Rfe1 Ba3 Ba5 b6 Bc3 Na6 Be4 Bd7 Nd2 (Depth: 19)\\n[White POV]: 49.9% cp:-1 Bd6 c3 O-O Bd3 c5 d5 Re8+ Ne2 Bg4 c4 g6 f3 Bd7 Kf2 Na6 a3 Nb4 Bb1 (Depth: 19)\\n[White POV]: 50.6% cp:5 Be6 Nf3 c5 Be3 cxd4 Qxd4 Bb4+ c3 Qxd4 Bxd4 Ba3 Bb5+ Nc6 Ke2 O-O-O Rhd1 Nxd4+ Nxd4 Bd6 (Depth: 19)\",\n",
    "      \"eval\": \"50.62496744995104\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"move_type\": \"mistake\",\n",
    "    \"move_number\": 1,\n",
    "    \"before\": {\n",
    "      \"fen\": \"r4r1k/pp1nQppp/1qp2p2/5b2/2BP4/1PP5/P2B1PPP/R3K1NR w KQ - 3 13\",\n",
    "      \"stockfish_analysis\": \"[White POV]: 91.8% cp:482 Ne2 Rae8 Qa3 Qd8 O-O Nb6 Rfe1 Nxc4 bxc4 Bd3 Nf4 Bxc4 Qc5 b5 Rxe8 Rxe8 Qxc6 a5 Qf3 (Depth: 18)\\n[White POV]: 90.8% cp:458 Qa3 Rfe8+ Ne2 Qd8 O-O Nb6 Ng3 Nxc4 bxc4 Bd3 Rfe1 Rxe1+ Bxe1 Bxc4 Qc5 b5 a4 a5 Nf5 Be6 (Depth: 18)\\n[White POV]: 89.5% cp:428 Qd6 Qa5 Qg3 Qa3 Bc1 Rfe8+ Ne2 Qe7 Qf3 Be4 Qe3 Bxg2 Qxe7 Rxe7 Rg1 Be4 Ng3 Rae8 Be2 (Depth: 18)\",\n",
    "      \"eval\": \"91.75866818720871\"\n",
    "    },\n",
    "    \"after\": {\n",
    "      \"fen\": \"r4r1k/pp1nQppp/1qp2p2/5b2/2BP4/1PP5/P2B1PPP/2KR2NR b - - 4 13\",\n",
    "      \"stockfish_analysis\": \"[White POV]: 81.5% cp:296 c5 dxc5 Qa5 Kb2 Rfe8 b4 Qa4 Bb3 Qxb3+ axb3 Rxe7 Nf3 Bg4 h3 Bxf3 gxf3 Ne5 Be3 Rae8 c4 (Depth: 18)\\n[White POV]: 83.9% cp:330 Qa5 Kb2 b5 Bf1 c5 Nf3 h6 Re1 c4 Qa3 Qc7 Ka1 b4 Qxb4 cxb3 Qxb3 (Depth: 18)\\n[White POV]: 85.0% cp:347 Qc7 Qe3 Rfe8 Qf4 Qa5 Kb2 Bg6 Bf1 b5 c4 b4 Nf3 Qa3+ Ka1 a5 Bc1 (Depth: 18)\",\n",
    "      \"eval\": \"85.00508583359613\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"move_type\": \"normal\",\n",
    "    \"move_number\": 2,\n",
    "    \"before\": {\n",
    "      \"fen\": \"r4r1k/2qn1ppp/2p2p2/pp3b2/3P2P1/QPP2N1P/P2B1P2/2KR1B1R b - - 0 18\",\n",
    "      \"stockfish_analysis\": \"[White POV]: 76.0% cp:230 b4 Qb2 Be4 Bg2 a4 Rhe1 axb3 axb3 bxc3 Bxc3 Bd5 Bh1 Rfb8 Nd2 Bxh1 Rxh1 Nb6 Qc2 h6 Rde1 (Depth: 20)\\n[White POV]: 76.0% cp:231 Be4 Bg2 b4 Qb2 a4 Rhe1 axb3 axb3 bxc3 Bxc3 Bd5 Bh1 Rfb8 Nd2 Bxh1 Rxh1 Nb6 Qc2 h6 Rhe1 (Depth: 20)\\n[White POV]: 79.5% cp:271 Be6 Qb2 c5 dxc5 Nxc5 Nd4 b4 c4 Rfb8 Be3 a4 Nxe6 Nxb3+ Kb1 fxe6 Be2 a3 Qxb3 Qb7 (Depth: 20)\",\n",
    "      \"eval\": \"79.49458649106352\"\n",
    "    },\n",
    "    \"after\": {\n",
    "      \"fen\": \"r4r1k/2qn1ppp/2p2p2/pp6/3Pb1P1/QPP2N1P/P2B1P2/2KR1B1R w - - 1 19\",\n",
    "      \"stockfish_analysis\": \"[White POV]: 75.3% cp:223 Bg2 b4 Qb2 a4 Rhe1 axb3 axb3 f5 cxb4 Nf6 Rxe4 Nxe4 Ne5 Nxf2 Rf1 f6 Rxf2 fxe5 Rxf5 Rxf5 (Depth: 21)\\n[White POV]: 62.4% cp:101 Be2 c5 Rhe1 Qb7 Be3 Bxf3 Bxf3 Qxf3 dxc5 Ne5 Qb2 Qxh3 Qe2 Qxg4 Qxb5 h5 Rd6 Kg8 c6 Rfc8 (Depth: 21)\\n[White POV]: 40.5% cp:-77 Rg1 Bxf3 Re1 a4 b4 Bd5 c4 bxc4 Qg3 Qb7 g5 Rae8 Be2 fxg5 Qxg5 Rg8 Bd1 Be4 Bc3 f6 (Depth: 21)\",\n",
    "      \"eval\": \"75.30600904213377\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"move_type\": \"blunder\",\n",
    "    \"move_number\": 2,\n",
    "    \"before\": {\n",
    "      \"fen\": \"r4r1k/2qn1ppp/2p2p2/p7/1p1Pb1P1/QPP2N1P/P2B1PB1/2KR3R w - - 0 20\",\n",
    "      \"stockfish_analysis\": \"[White POV]: 76.0% cp:230 Qb2 a4 Rhe1 f5 gxf5 axb3 axb3 bxc3 Bxc3 Bd5 Bh1 f6 Qc2 Nb6 Rd3 Rfb8 Rde3 (Depth: 18)\\n[White POV]: 35.5% cp:-119 cxb4 axb4 Qxb4 Rxa2 Rde1 c5 Qc3 Rc2+ Qxc2 Bxc2 Kxc2 cxd4+ Kb2 Qb6 Rc1 Rb8 b4 d3 Rhf1 Qb5 (Depth: 18)\\n[White POV]: 13.6% cp:-369 Rhe1 Bxf3 cxb4 Bxg2 bxa5 Bf3 Qe7 Bxd1 Kxd1 Qc8 d5 cxd5 a3 (Depth: 18)\",\n",
    "      \"eval\": \"75.9510916949111\"\n",
    "    },\n",
    "    \"after\": {\n",
    "      \"fen\": \"r4r1k/2qn1ppp/2p2p2/p7/1P1Pb1P1/QP3N1P/P2B1PB1/2KR3R b - - 0 20\",\n",
    "      \"stockfish_analysis\": \"[White POV]: 38.3% cp:-95 axb4 Qxb4 Rxa2 Rde1 c5 Qc3 Bd5 Re7 Qd6 Rhe1 cxd4 Nxd4 Bxg2 Re8 Ra8 Nb5 Qa6 Nc7 Qa3+ Kd1 (Depth: 21)\\n[White POV]: 81.4% cp:295 Rfc8 bxa5 c5 Kb2 Qb7 Rhe1 Bxf3 Bxf3 Qxf3 b4 Qb7 dxc5 Nxc5 Ka1 Ne4 Be3 h6 Rd4 Rc2 Rc1 (Depth: 21)\\n[White POV]: 82.7% cp:313 a4 bxa4 Rfb8 Ne1 Bxg2 Nxg2 c5 Bf4 Qc6 b5 Qe6 Ne3 cxd4 Rxd4 Rxb5 Re1 (Depth: 21)\",\n",
    "      \"eval\": \"82.70696501038614\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"move_type\": \"blunder\",\n",
    "    \"move_number\": 3,\n",
    "    \"before\": {\n",
    "      \"fen\": \"7k/2q2ppp/5p2/2n5/1Q2R1PN/1P5P/2rB1P2/3K3R b - - 0 27\",\n",
    "      \"stockfish_analysis\": \"[White POV]: 95.5% cp:611 Rxd2+ Kxd2 Qd8+ Rd4 Nxb3+ Ke3 Nxd4 Qxd4 Qe7+ Kf3 h5 Kg2 g5 Nf3 hxg4 Qxg4 Kg7 Qg3 Qa7 (Depth: 20)\\n[White POV]: 97.1% cp:705 h5 Re8+ Kh7 Kxc2 Qc6 Re3 Ne6+ Kb2 Qxh1 Qe4+ Qxe4 Rxe4 hxg4 hxg4 Nc5 Re3 g6 Ka3 g5 Nf3 (Depth: 20)\\n[White POV]: 97.5% cp:730 Qc8 Kxc2 Nxe4+ Qc4 Nc5 Re1 h5 Nf5 Kh7 Re3 Qa8 Qxc5 hxg4 hxg4 Qa2+ Kd3 Qxb3+ Ke2 (Depth: 20)\",\n",
    "      \"eval\": \"97.46672967731283\"\n",
    "    },\n",
    "    \"after\": {\n",
    "      \"fen\": \"7k/2q2ppp/5p2/8/1Q2n1PN/1P5P/2rB1P2/3K3R w - - 0 28\",\n",
    "      \"stockfish_analysis\": \"[White POV]: 100.0% cp:inf Qf8# (Depth: 20)\\n[White POV]: 96.2% cp:648 Qxe4 Rxd2+ Kxd2 g6 Nf3 Qa5+ Ke2 Qa2+ Kf1 Qxb3 Kg2 h5 Rb1 Qa2 Rb8+ Kg7 g5 fxg5 Qe5+ f6 (Depth: 20)\\n[White POV]: 94.8% cp:580 Bc3 Nxc3+ Kxc2 Nd5+ Qc4 Nb4+ Kb2 Qb7 Rc1 g6 Nxg6+ fxg6 Qc8+ Qxc8 Rxc8+ Kg7 Ka3 Nd3 f3 h5 (Depth: 20)\",\n",
    "      \"eval\": \"100.0\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"move_type\": \"mistake\",\n",
    "    \"move_number\": 2,\n",
    "    \"before\": {\n",
    "      \"fen\": \"r1b1r1k1/1p1n1ppp/p3pq2/3p4/P1PP4/3BPN2/5PPP/1R1Q1RK1 b - - 0 14\",\n",
    "      \"stockfish_analysis\": \"[White POV]: 55.8% cp:47 dxc4 Bxc4 b6 d5 e5 Qb3 Rb8 Qa3 e4 Nd4 Bb7 Nc6 Bxc6 dxc6 Qxc6 Bxa6 Red8 Bb5 Qe6 Rfd1 (Depth: 19)\\n[White POV]: 67.4% cp:145 Qd8 Qc2 g6 c5 Qc7 Rfc1 e5 Be2 e4 Nd2 Re6 Bg4 f5 Be2 Nf6 Qb3 Rc6 h3 f4 exf4 (Depth: 19)\\n[White POV]: 68.7% cp:157 g6 cxd5 exd5 Qb3 Qd6 Rfc1 Rb8 Qc2 Nf6 Qc7 Qxc7 Rxc7 Be6 Ng5 Bd7 Rbxb7 Rxb7 Rxb7 Rc8 h3 (Depth: 19)\",\n",
    "      \"eval\": \"68.67567266439018\"\n",
    "    },\n",
    "    \"after\": {\n",
    "      \"fen\": \"r1b1r1k1/1p1n1ppp/p4q2/3pp3/P1PP4/3BPN2/5PPP/1R1Q1RK1 w - - 0 15\",\n",
    "      \"stockfish_analysis\": \"[White POV]: 71.2% cp:181 Nxe5 Nxe5 dxe5 Rxe5 cxd5 Bf5 Qb3 Bxd3 Qxd3 Rd8 Rfd1 Rexd5 Qxd5 Rxd5 Rxd5 h5 Rd4 Qc6 h3 a5 (Depth: 19)\\n[White POV]: 70.9% cp:178 dxe5 Nxe5 Nxe5 Rxe5 cxd5 Bf5 Qb3 Bxd3 Qxd3 Rd8 Rfd1 Rexd5 Qxd5 Rxd5 Rxd5 h5 Rd4 Qc6 Rd8+ Kh7 (Depth: 19)\\n[White POV]: 56.2% cp:50 Be2 dxc4 Bxc4 exd4 Nxd4 Rd8 Qc2 Ne5 Be2 Rd6 Rfd1 Nc6 Nxc6 Rxc6 Qe4 (Depth: 19)\",\n",
    "      \"eval\": \"71.19759194127936\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"move_type\": \"normal\",\n",
    "    \"move_number\": 3,\n",
    "    \"before\": {\n",
    "      \"fen\": \"r1b1r1k1/1p1n1ppp/p4q2/3P4/P2Pp3/3BPN2/5PPP/1R1Q1RK1 w - - 0 16\",\n",
    "      \"stockfish_analysis\": \"[White POV]: 31.3% cp:-157 Be2 exf3 Bxf3 Nf8 e4 b5 e5 Qb6 Be4 Bb7 Qd2 Rab8 axb5 axb5 f4 (Depth: 18)\\n[White POV]: 30.8% cp:-162 Bxe4 Rxe4 Re1 Rb8 Qc2 Re8 Qc7 b5 axb5 axb5 Ne5 Qb6 Qc6 Qd8 Qd6 Rb6 Nc6 Qf6 Qc7 Rb7 (Depth: 18)\\n[White POV]: 26.1% cp:-208 Bc4 exf3 Qxf3 Qg6 Qg3 Qc2 Qc7 Nf6 d6 Qg6 f3 Bh3 Qxf7+ Qxf7 Bxf7+ Kxf7 gxh3 Rxe3 Rxb7+ Kg8 (Depth: 18)\",\n",
    "      \"eval\": \"31.324327335609823\"\n",
    "    },\n",
    "    \"after\": {\n",
    "      \"fen\": \"r1b1r1k1/1p1n1ppp/p4q2/3P4/P2PB3/4PN2/5PPP/1R1Q1RK1 b - - 0 16\",\n",
    "      \"stockfish_analysis\": \"[White POV]: 30.2% cp:-168 Rxe4 Re1 Rb8 Qc2 Re8 e4 b5 e5 Qg6 Qc7 Rb7 Qc6 Rb6 Qc7 bxa4 Ra1 Rb7 Qc6 Rb8 e6 (Depth: 19)\\n[White POV]: 83.7% cp:327 Qd8 Bd3 Nf6 Qc2 Qxd5 Ne5 Be6 Bc4 Qe4 Qxe4 Nxe4 Bxe6 Rxe6 Rxb7 f6 Nd3 Rc6 h4 a5 Rfb1 (Depth: 19)\\n[White POV]: 85.2% cp:350 Qd6 Nd2 b5 Bf3 Nb6 axb5 axb5 Re1 Bd7 e4 Nxd5 exd5 Rxe1+ Qxe1 Re8 Qf1 (Depth: 19)\",\n",
    "      \"eval\": \"85.19528019683105\"\n",
    "    }\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111e7535",
   "metadata": {
    "papermill": {
     "duration": 0.053857,
     "end_time": "2025-12-16T16:39:14.962258",
     "exception": false,
     "start_time": "2025-12-16T16:39:14.908401",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## testing the model with huggingface format **on gpu**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2461e953",
   "metadata": {
    "papermill": {
     "duration": 0.05387,
     "end_time": "2025-12-16T16:39:15.069685",
     "exception": false,
     "start_time": "2025-12-16T16:39:15.015815",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### simple method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4968af4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T16:39:15.178130Z",
     "iopub.status.busy": "2025-12-16T16:39:15.177755Z",
     "iopub.status.idle": "2025-12-16T16:39:25.243311Z",
     "shell.execute_reply": "2025-12-16T16:39:25.242363Z"
    },
    "papermill": {
     "duration": 10.121322,
     "end_time": "2025-12-16T16:39:25.244900",
     "exception": false,
     "start_time": "2025-12-16T16:39:15.123578",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Analysis shows that the best move is 1. Rf4, with the following possible variations: 1.... Kxf4; 2.Rxf4, Kf3; 3.Rg4, Kxg4; 4.Rxg4, Kf3; 5.Rf4, Kxg4; 6.Rg4, Kf3; 7.Rf4, Kxg4; 8.Rg4, Kf5; 9.Rf4, Kxg4; 10.Rg4, Kf5; 11 .Rg3, Kxg5; 12 .Rxg5, Kf6; 13 .Rg3, Ke7; 14.Rg2, Kd6; 15 .Rg1, Kc7; 16.Rg3, Kd6; 17 .Rg1, Kc7; 18 .Rg2, Kd6; 19.Rg3, Kc7; 20.Rg4, Kd6; 21 .Rg3, Kd7; 22 .\n"
     ]
    }
   ],
   "source": [
    "print(simple_method(samples[6],chat_qwen3_direct))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5702dd77",
   "metadata": {
    "papermill": {
     "duration": 0.053118,
     "end_time": "2025-12-16T16:39:25.352479",
     "exception": false,
     "start_time": "2025-12-16T16:39:25.299361",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### chained method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b1bf444",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T16:39:25.465535Z",
     "iopub.status.busy": "2025-12-16T16:39:25.464841Z",
     "iopub.status.idle": "2025-12-16T16:39:55.790096Z",
     "shell.execute_reply": "2025-12-16T16:39:55.789281Z"
    },
    "papermill": {
     "duration": 30.519979,
     "end_time": "2025-12-16T16:39:55.930687",
     "exception": false,
     "start_time": "2025-12-16T16:39:25.410708",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"before_llm\": \"'Humiliating the work of the noble b4-bishop.\\n'x---------------------------------PART ONE I THE CONCEPT OF IMB ALANCES 31 --\\\"--PART TWO - THE CONCEPT OF IMB ALANCES 33 -----------------------------------PART THREE I ROOKS 35 -----------------------------------PART FOUR I PSY CHOLOGICAL MEANDERINGS 37 -----------------------------------PART FIVE - TEST 36 --PART FIVE - TEST 37 --PART FIVE - TEST 38 --PART FIVE - TEST 39 --PART FIVE - TEST 40 --PART FIVE - TEST 41 --PART FIVE - TEST 42 --PART FIVE - TEST 43 --PART FIVE - TEST 44 --PART FIVE - TEST 45 --PART FIVE - TEST 46 --PART FIVE - TEST 47 --PART FIVE - TEST 48 --PART FIVE - TEST 49 --PART FIVE - TEST 50 --PART FIVE - TEST 51 'i:i.\",\n",
      "  \"after_llm\": \"\\\"A waiting and observing man.\\\"\\nAs the saying goes, \\\"As the saying goes, so shall ye learn to play chess.\\\"\\nThe chessboard is a place of instruction for instruction:\\nIt is a place of instruction for composers (to compose), and a place of disputation (and ridicule).\\nThe beginner learns by watching a master play and imitate (the style and habits) of the master. He learns by hearing and speaking words. And he learns by seeing and feeling the way both sides walk in sharp time. And he learns by doing (and observing) the fine techniques of both sides. And what is the joy of playing chess? Not only does it profit the player who understands the game, but also does it make him feel good-humoured and thus increases his self-knowledge and confidence.\\nIt is not fitting for a player that is an expert or master in a particular field, to make a learning expedition in the midst of unlearned chess. The value of a player's knowledge and ability to judge a game quickly and accurately is directly in proportion to his playing strength. Do not try to ascertain the worth of a player's chess knowledge by his ability to judge a game. The same cannot be said about a player's playing strength.\",\n",
      "  \"final_analysis\": \"Free Range Gauntlet\\nFree Range Kniguts\\nFree Range Boiling Water\\nFree Range Fire\\nFree Range Scorching Water\\nFree Range, All Capablanca,\\nFree Range, Capablanca,\\nFree Range Rubbish,\\nFree Range, Alekhine,\\nFree Range, Nimzo-Indian,\\nFree Range, Piquant,\\nFree Range, Alekhine, Piquant,\\nFree Range, Sad.\\nFree Range, Gracious, Most Merciful.\\nFree Range, King, Queen, Rook, and Bis hop.\\nFree Range, King, Queen, Rook, and Bishop.\\nFree Range, King, Queen, Rook, and Bishop.\\nFree Range, King, Queen, Rook, and Bishop.\\nFree Range, King, Queen, Rook, and Bishop.\\nFree Range, King, Queen, Rook, and Bishop.\\nFree Range, King, Queen, Rook, and Bishop.\\nFree Range, King, Queen, Rook, and Bishop.\\nFree Range, King, Queen, Rook, and Bishop.\\nFree Range, King, Queen, Rook, and Bishop.\\nFree Range, King, Queen, Rook, and Bishop.\\nFree Range, King, Queen, Rook, and Bishop.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "result = full_llm_pipeline(samples[6], chat_qwen3_direct)\n",
    "print(json.dumps(result, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623acbac",
   "metadata": {
    "papermill": {
     "duration": 0.052854,
     "end_time": "2025-12-16T16:39:56.035934",
     "exception": false,
     "start_time": "2025-12-16T16:39:55.983080",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## testing the model with gguf format **on cpu**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd99a0f4",
   "metadata": {
    "papermill": {
     "duration": 0.053932,
     "end_time": "2025-12-16T16:39:56.145816",
     "exception": false,
     "start_time": "2025-12-16T16:39:56.091884",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### simple method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "afdb4fc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T16:39:56.253366Z",
     "iopub.status.busy": "2025-12-16T16:39:56.252856Z",
     "iopub.status.idle": "2025-12-16T16:40:08.140661Z",
     "shell.execute_reply": "2025-12-16T16:40:08.139803Z"
    },
    "papermill": {
     "duration": 11.942667,
     "end_time": "2025-12-16T16:40:08.142036",
     "exception": false,
     "start_time": "2025-12-16T16:39:56.199369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 111.0-0-0\n",
      "' eval': '97.26672967731283 101.0-0-0 102.0-0-0 103.\n"
     ]
    }
   ],
   "source": [
    "print(simple_method(samples[6],chat_gguf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e95cbb3",
   "metadata": {
    "papermill": {
     "duration": 0.053016,
     "end_time": "2025-12-16T16:40:08.253583",
     "exception": false,
     "start_time": "2025-12-16T16:40:08.200567",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### chained method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6bb691af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T16:40:08.360813Z",
     "iopub.status.busy": "2025-12-16T16:40:08.360158Z",
     "iopub.status.idle": "2025-12-16T16:40:31.219980Z",
     "shell.execute_reply": "2025-12-16T16:40:31.219123Z"
    },
    "papermill": {
     "duration": 22.914351,
     "end_time": "2025-12-16T16:40:31.221125",
     "exception": false,
     "start_time": "2025-12-16T16:40:08.306774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"before_llm\": \": a chess engine that, by its very nature, will instantly and permanently be-\\nA useful study tool for the masses of players who want to know how to play a given position.\",\n",
      "  \"after_llm\": \"pro: the engine will do a best job at the appropriate moment.\\nIt is most fitting that the Rook should be placed on the square directly in front of the pawn, that it may be attacked by the pawn, that the pawn may be advanced and that the pawn be removed from the main theatre of events.\",\n",
      "  \"final_analysis\": \"(100% manned).\\n>> 376 HOW TO REASSESS YOUR CHESS - 4TH EDITION >;\\n>> 128 as = =:::>;\\n> ;; = =:::> ;> ;; ;. ;; ;.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "result = full_llm_pipeline(samples[6], chat_gguf)\n",
    "print(json.dumps(result, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b217b432",
   "metadata": {
    "papermill": {
     "duration": 0.053433,
     "end_time": "2025-12-16T16:40:31.329581",
     "exception": false,
     "start_time": "2025-12-16T16:40:31.276148",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 3923857,
     "sourceId": 6823361,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8875966,
     "sourceId": 14070536,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8695.355824,
   "end_time": "2025-12-16T16:40:34.504519",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-16T14:15:39.148695",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "049d68d716d94c11bccdd0ba13e58abb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "0853be1f027f4f9ea0f76424b2569e28": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "09a65d1ccb504629ba87bb865c31d646": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "0de351e80ab04f179707c8ae045120d2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "0e3bd1c4fdd4487e896ccd541f97f91d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_54f208f43c8d46dd8a7e8d52d57dbf2c",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_62c940dc538249d983fee8ce871af781",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "0f5aa9c137384aa0a6cdbd9b7e66b1d1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "13e06164bb744cfcbd9df59cd8e054ec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "14d46a53924740fd878d733cd0760d2f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1813af9f697f44718096d0b36d81f717": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1bd2401839a84035a756d1a631ff7492": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1de0ddff382841b9bfdf3d1af5d5455a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "202c239a95e741fcba84846a42bd41ad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "271cd85208e0400fb7e9a837db5b10f0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1de0ddff382841b9bfdf3d1af5d5455a",
       "max": 1503300328.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_0853be1f027f4f9ea0f76424b2569e28",
       "tabbable": null,
       "tooltip": null,
       "value": 1503300328.0
      }
     },
     "28453e5ba5eb4898854f338b46862208": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2f0c64f963c945ceae7ed2aeb2ec41d2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f0dd8e8c066240608e18c856c43b692e",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_0de351e80ab04f179707c8ae045120d2",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡1.67M/?â€‡[00:00&lt;00:00,â€‡24.6MB/s]"
      }
     },
     "348febb3b3b14132ba7bf4461e7f62c3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e614bcc5240249e49d5956d5a4218ef5",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_b9ff63cdd3ae49fba342e3542fd699e2",
       "tabbable": null,
       "tooltip": null,
       "value": "tokenizer_config.json:â€‡"
      }
     },
     "363ef630c1fc4ff0b8d1b8b996dfcfbe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "3815f6ae70f64fc78d99035a1088301f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3c82992689194e489b0e1956f807f225": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3df507c4277549a8a1d5b26e2e433779": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4a899d32de2e46d5b2c760d5c2f4fcef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e94475fb3e5e4ab19f7ae4ed763df81e",
        "IPY_MODEL_e0a2d91418414b829919d841dc99a2fe",
        "IPY_MODEL_b8e2a0c7ea3a4985aca8b3b935111d3c"
       ],
       "layout": "IPY_MODEL_3df507c4277549a8a1d5b26e2e433779",
       "tabbable": null,
       "tooltip": null
      }
     },
     "4d7841e7d03c4212a696330dbecf2e7d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4fa2ceebb8cf418a91c3fdbde1f7e1ff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "52a25c7f063a49a399d0ecdf4ab6d4ae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c8083f0377114147a274e1f589c8966c",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_202c239a95e741fcba84846a42bd41ad",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "541c0b2a33af4bd6b7caaf24d5254c80": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "54f208f43c8d46dd8a7e8d52d57dbf2c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "567d3ed6fa70489d83fe28795aae6328": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e6ba3a29ee264bc98bc806d36b9cb94e",
        "IPY_MODEL_6b30e9fb77ea4c33bfb6b1688277020b",
        "IPY_MODEL_f3953988b7fe4be1b62c4aa7c93f0092"
       ],
       "layout": "IPY_MODEL_f1b1088c68d44447893cb6f55bc901c8",
       "tabbable": null,
       "tooltip": null
      }
     },
     "593fdf92be234e4291d136980d6a1951": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f0946f8397f741afb5474cc48978237d",
        "IPY_MODEL_e6207d4471c2404baabdac147017bdc6",
        "IPY_MODEL_b95c87c2b1884e46b58386d2f44aad8b"
       ],
       "layout": "IPY_MODEL_80cede9dbc3c4922bfffd0942af4b61b",
       "tabbable": null,
       "tooltip": null
      }
     },
     "5d28542f6aa64370a9b5d5f9d17d4934": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "62c940dc538249d983fee8ce871af781": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "6359802ba2384ce4955bc3df70bc68c8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_68d779ea6c42423c838721ed320d62e1",
        "IPY_MODEL_271cd85208e0400fb7e9a837db5b10f0",
        "IPY_MODEL_e49d7e6c59e14f5383bc4d8daba83653"
       ],
       "layout": "IPY_MODEL_93d87368e21b4cff83e00635e91bd5a3",
       "tabbable": null,
       "tooltip": null
      }
     },
     "68d779ea6c42423c838721ed320d62e1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_eeef2d5b46a1490f849c4babffdd5950",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_a684511d046a478094be897c39fd6687",
       "tabbable": null,
       "tooltip": null,
       "value": "model.safetensors:â€‡100%"
      }
     },
     "6aae745cfe0144ed9cc41bfef0f6d309": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "6b30e9fb77ea4c33bfb6b1688277020b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1bd2401839a84035a756d1a631ff7492",
       "max": 239.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_049d68d716d94c11bccdd0ba13e58abb",
       "tabbable": null,
       "tooltip": null,
       "value": 239.0
      }
     },
     "7003bd05547b4ee787b5420543deffe1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7004d6dd0e1f4940ac9ae39e6ba53306": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "73a494991c314a598c9dc52ac593b546": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_de044d45f7f243c9b5c0e03765ed2c57",
        "IPY_MODEL_52a25c7f063a49a399d0ecdf4ab6d4ae",
        "IPY_MODEL_b53b0fc5a82c4bdf85b6ac6e88cc58fe"
       ],
       "layout": "IPY_MODEL_c05ebf083b0e4751acc9ab452275a6a2",
       "tabbable": null,
       "tooltip": null
      }
     },
     "7a9d55b16bf24f678bd5944eafd18b3c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "80cede9dbc3c4922bfffd0942af4b61b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "866ad452277441bf9b5dee66e79389b6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_348febb3b3b14132ba7bf4461e7f62c3",
        "IPY_MODEL_e7c6e7fe6f634f439387c9680bf393e8",
        "IPY_MODEL_88aa7fd0c17f49168ff49383056a7293"
       ],
       "layout": "IPY_MODEL_88ef65ba34b9480f9d236dbe52d47cd5",
       "tabbable": null,
       "tooltip": null
      }
     },
     "88aa7fd0c17f49168ff49383056a7293": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3c82992689194e489b0e1956f807f225",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_ee834b69cf804fbc85bcf2c529f09690",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡9.73k/?â€‡[00:00&lt;00:00,â€‡940kB/s]"
      }
     },
     "88ef65ba34b9480f9d236dbe52d47cd5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "89a482999efa4b1f9c84199d564a25e0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8f53bde4d16d4f789346deef6f135439": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "93d87368e21b4cff83e00635e91bd5a3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a1d20086caf84e7ea09226329e6aa870": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e5001f45e4c04c9da071f06dcd3e85f9",
        "IPY_MODEL_0e3bd1c4fdd4487e896ccd541f97f91d",
        "IPY_MODEL_2f0c64f963c945ceae7ed2aeb2ec41d2"
       ],
       "layout": "IPY_MODEL_1813af9f697f44718096d0b36d81f717",
       "tabbable": null,
       "tooltip": null
      }
     },
     "a684511d046a478094be897c39fd6687": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b27e2e9957044ca6aac8264bd70090f9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b370842d070548e7b029c192227c4eb2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b53b0fc5a82c4bdf85b6ac6e88cc58fe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_89a482999efa4b1f9c84199d564a25e0",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_4fa2ceebb8cf418a91c3fdbde1f7e1ff",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡2.78M/?â€‡[00:00&lt;00:00,â€‡11.2MB/s]"
      }
     },
     "b8e2a0c7ea3a4985aca8b3b935111d3c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0f5aa9c137384aa0a6cdbd9b7e66b1d1",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_13e06164bb744cfcbd9df59cd8e054ec",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡11.4M/11.4Mâ€‡[00:02&lt;00:00,â€‡6.22MB/s]"
      }
     },
     "b95c87c2b1884e46b58386d2f44aad8b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f834675ae205482eb1bc32ec064e25c5",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_14d46a53924740fd878d733cd0760d2f",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡726/726â€‡[00:00&lt;00:00,â€‡103kB/s]"
      }
     },
     "b9ff63cdd3ae49fba342e3542fd699e2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "be0c99896e3e45d9aa51c367d6746693": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "bf9e4bdfaba54fe59a49369cec052296": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c05ebf083b0e4751acc9ab452275a6a2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c8083f0377114147a274e1f589c8966c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "cb1e89b28b674e75a9f0751ff84105b0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "de044d45f7f243c9b5c0e03765ed2c57": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b27e2e9957044ca6aac8264bd70090f9",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_8f53bde4d16d4f789346deef6f135439",
       "tabbable": null,
       "tooltip": null,
       "value": "vocab.json:â€‡"
      }
     },
     "e0a2d91418414b829919d841dc99a2fe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5d28542f6aa64370a9b5d5f9d17d4934",
       "max": 11422654.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_6aae745cfe0144ed9cc41bfef0f6d309",
       "tabbable": null,
       "tooltip": null,
       "value": 11422654.0
      }
     },
     "e34c2d5693de4afa9eb77b6e0404b93e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "e3726706c51242d9a7a859e43cb515cf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e49d7e6c59e14f5383bc4d8daba83653": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b370842d070548e7b029c192227c4eb2",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_e3726706c51242d9a7a859e43cb515cf",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡1.50G/1.50Gâ€‡[00:05&lt;00:00,â€‡998MB/s]"
      }
     },
     "e5001f45e4c04c9da071f06dcd3e85f9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7003bd05547b4ee787b5420543deffe1",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_7a9d55b16bf24f678bd5944eafd18b3c",
       "tabbable": null,
       "tooltip": null,
       "value": "merges.txt:â€‡"
      }
     },
     "e614bcc5240249e49d5956d5a4218ef5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e6207d4471c2404baabdac147017bdc6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_28453e5ba5eb4898854f338b46862208",
       "max": 726.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_be0c99896e3e45d9aa51c367d6746693",
       "tabbable": null,
       "tooltip": null,
       "value": 726.0
      }
     },
     "e6ba3a29ee264bc98bc806d36b9cb94e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_541c0b2a33af4bd6b7caaf24d5254c80",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_e8a07ab3fb9643aa8d12656a4dc785e3",
       "tabbable": null,
       "tooltip": null,
       "value": "generation_config.json:â€‡100%"
      }
     },
     "e7c6e7fe6f634f439387c9680bf393e8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_363ef630c1fc4ff0b8d1b8b996dfcfbe",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e34c2d5693de4afa9eb77b6e0404b93e",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "e8a07ab3fb9643aa8d12656a4dc785e3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e94475fb3e5e4ab19f7ae4ed763df81e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7004d6dd0e1f4940ac9ae39e6ba53306",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_09a65d1ccb504629ba87bb865c31d646",
       "tabbable": null,
       "tooltip": null,
       "value": "tokenizer.json:â€‡100%"
      }
     },
     "ee834b69cf804fbc85bcf2c529f09690": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "eeef2d5b46a1490f849c4babffdd5950": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f0946f8397f741afb5474cc48978237d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_bf9e4bdfaba54fe59a49369cec052296",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_3815f6ae70f64fc78d99035a1088301f",
       "tabbable": null,
       "tooltip": null,
       "value": "config.json:â€‡100%"
      }
     },
     "f0dd8e8c066240608e18c856c43b692e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f1b1088c68d44447893cb6f55bc901c8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f3953988b7fe4be1b62c4aa7c93f0092": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_cb1e89b28b674e75a9f0751ff84105b0",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_4d7841e7d03c4212a696330dbecf2e7d",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡239/239â€‡[00:00&lt;00:00,â€‡19.7kB/s]"
      }
     },
     "f834675ae205482eb1bc32ec064e25c5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
